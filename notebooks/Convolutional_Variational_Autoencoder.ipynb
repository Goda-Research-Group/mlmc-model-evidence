{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "## Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfIk2es3hJEd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.models.convolutional_variational_autoencoder import IWAE, IWAE_MLMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "## Load the MNIST dataset\n",
    "Each MNIST image is originally a vector of 784 integers, each of which is between 0-255 and represents the intensity of a pixel. We model each pixel with a Bernoulli distribution in our model, and we statically binarize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4fYMGxGhrna"
   },
   "outputs": [],
   "source": [
    "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFC2ghIdiZYE"
   },
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "    images = images.reshape((images.shape[0], 28, 28, 1)) / 255.\n",
    "    return np.where(images > .5, 1.0, 0.0).astype('float32')\n",
    "\n",
    "train_images = preprocess_images(train_images)\n",
    "test_images = preprocess_images(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S4PIDhoDLbsZ"
   },
   "outputs": [],
   "source": [
    "train_size = 60000\n",
    "batch_size = 32\n",
    "test_size = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIGN6ouoQxt3"
   },
   "source": [
    "## Use *tf.data* to batch and shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-yKCCQOoJ7cn"
   },
   "outputs": [],
   "source": [
    "train_dataset = (tf.data.Dataset.from_tensor_slices(train_images)\n",
    "                 .shuffle(train_size).batch(batch_size))\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices(test_images)\n",
    "                .shuffle(test_size).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NS2GWywBbAWo"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dataset, loss):\n",
    "    for train_x in train_dataset:\n",
    "        model.train_step(train_x, loss=loss)\n",
    "\n",
    "def train_and_plot(model, train_dataset, test_dataset, epochs = 10, loss='elbo'):\n",
    "    num_examples_to_generate = 16\n",
    "\n",
    "    # keeping the random vector constant for generation (prediction) so\n",
    "    # it will be easier to see the improvement.\n",
    "    random_vector_for_generation = tf.random.normal(\n",
    "        shape=[num_examples_to_generate, latent_dim])\n",
    "\n",
    "    # Pick a sample of the test set for generating output images\n",
    "    assert batch_size >= num_examples_to_generate\n",
    "    for test_batch in test_dataset.take(1):\n",
    "        test_sample = test_batch[0:num_examples_to_generate, :, :, :]\n",
    "\n",
    "    model.generate_images(0, test_sample)\n",
    "    plt.show()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        start_time = time.time()\n",
    "        train_epoch(model, train_dataset, loss)\n",
    "        end_time = time.time()\n",
    "\n",
    "        elbo = np.mean([model.compute_elbo(test_x) for test_x in test_dataset])\n",
    "        display.clear_output(wait=False)\n",
    "        print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'\n",
    "                .format(epoch, elbo, end_time - start_time))\n",
    "        model.generate_images(epoch, test_sample)\n",
    "        # tight_layout minimizes the overlap between 2 sub-plots\n",
    "        #plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence(model, test_dataset, grad=False):\n",
    "    \n",
    "    def get_grad(x, L):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss =  - model._compute_diwelbos(x, L)\n",
    "            gradients = tape.gradient(loss, model._decoder.trainable_variables)\n",
    "        gradients = tf.concat([tf.reshape(g,[-1]) for g in gradients], axis=0)\n",
    "        return gradients\n",
    "    \n",
    "    def get_L2_norm(L):\n",
    "        L2 = 0\n",
    "        for x in tmp_dataset:\n",
    "            if grad==False:  L2 += model._compute_diwelbos(x, L)**2\n",
    "            elif grad==True: L2 += get_grad(x, L)**2\n",
    "        return L2/100\n",
    "    \n",
    "    tmp_dataset = (tf.data.Dataset.from_tensor_slices(test_images[:100])\n",
    "                    .shuffle(100).batch(1))\n",
    "    \n",
    "    # evaluation of L2 norms\n",
    "    levels = np.arange(10)\n",
    "    res = [get_L2_norm(l) for l in levels]\n",
    "    \n",
    "    # plot\n",
    "    plt.plot([2**l for l in levels], res)\n",
    "    plt.plot([2**l for l in levels], [res[1]*2.**(1.5-l) for l in levels], c='gray')\n",
    "    plt.plot([2**l for l in levels], [res[1]*2.**(1.5-2*l) for l in levels], c='gray')\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.legend([r'$\\sqrt{E[||\\Delta \\nabla IWELBO||_2^2]}$', r'$O(2^{-\\ell})$', r'$O(2^{-2\\ell})$'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the dimensionality of the latent space to a plane for visualization later\n",
    "latent_dim = 3\n",
    "model = IWAE_MLMC(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,5])\n",
    "plt.subplot(1,2,1)\n",
    "plot_convergence(model, test_dataset, grad=False)\n",
    "plt.subplot(1,2,2)\n",
    "plot_convergence(model, test_dataset, grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_and_plot(model, train_dataset, test_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,5])\n",
    "plt.subplot(1,2,1)\n",
    "plot_convergence(model, test_dataset, grad=False)\n",
    "plt.subplot(1,2,2)\n",
    "plot_convergence(model, test_dataset, grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.2 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "train_epoch(model, train_dataset, loss='elbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 19s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "train_epoch(model, train_dataset, loss='iwelbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min 57s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "train_epoch(model, train_dataset, loss='iwelbo_mlmc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min 52s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "train_epoch(model, train_dataset, loss='iwelbo_rmlmc')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cvae.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
