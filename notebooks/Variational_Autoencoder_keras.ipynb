{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・勾配で、計算量のプロットを作る\n",
    "・global parameter（n_latent, sess, mnistなど）を消す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "#import scipy.stats\n",
    "#import math\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "# Normalizing the images to the range of [0., 1.]\n",
    "train_images /= 255.\n",
    "test_images /= 255.\n",
    "\n",
    "# Binarization\n",
    "train_images[train_images >= .5] = 1.\n",
    "train_images[train_images < .5] = 0.\n",
    "test_images[test_images >= .5] = 1.\n",
    "test_images[test_images < .5] = 0.\n",
    "\n",
    "TRAIN_BUF = 60000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "TEST_BUF = 10000\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(TRAIN_BUF).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_images).shuffle(TEST_BUF).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_in = tf.placeholder(dtype=float_type, shape=[None, 28, 28], name='X')\n",
    "#X0 = tf.reshape(X_in,[-1, 1, 28, 28])\n",
    "drop_rate = 0.2\n",
    "#with tf.name_scope(\"decoder\"):\n",
    "#    n_beta = tf.Variable(1., 'n_beta', dtype=float_type) #variance of image output\n",
    "\n",
    "n_latent = 5\n",
    "dec_in_channels = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# hyper paremeters depending on models\n",
    "batch_size = 100\n",
    "n_MC = {}\n",
    "loss = {}\n",
    "optimizer = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility funcs\n",
    "float_type = np.float32\n",
    "\n",
    "tf_int32 = lambda x:tf.cast(x, tf.int32)\n",
    "tf_float = lambda x:tf.cast(x, float_type)\n",
    "\n",
    "def lrelu(x, alpha=0.3):\n",
    "    return tf.maximum(x, tf.multiply(x, alpha))\n",
    "\n",
    "def tf_reduce_logmeanexp(ary, axis=1):\n",
    "    n_MC = tf.shape(ary)[axis]\n",
    "    return tf.reduce_logsumexp(ary, axis=axis) - tf.log(tf_float(n_MC))\n",
    "\n",
    "def tf_sum(_list):\n",
    "    S = 0\n",
    "    for val in _list:\n",
    "        S += val\n",
    "    return val\n",
    "\n",
    "def save(data_obj, name):\n",
    "    with open(name, mode='wb') as f:\n",
    "        pickle.dump(data_obj, f)\n",
    "        \n",
    "def load(name):\n",
    "    with open(name, mode='rb') as f:\n",
    "        data_obj = pickle.load(f)\n",
    "    return data_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, latent_dim, drop_rate):\n",
    "        \n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.drop_rate = drop_rate\n",
    "        \n",
    "        self.inference_net = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=4, strides=2, padding='same', activation=lrelu),\n",
    "            tf.keras.layers.Dropout(rate=drop_rate),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=4, strides=2, padding='same', activation=lrelu),\n",
    "            tf.keras.layers.Dropout(rate=drop_rate),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=4, strides=1, padding='same', activation=lrelu),\n",
    "            tf.keras.layers.Dropout(rate=drop_rate),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "        ])\n",
    "        \n",
    "        self.generative_net = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(latent_dim)),\n",
    "            tf.keras.layers.Dense(units=7*7*dec_in_channels//2, activation=lrelu),\n",
    "            tf.keras.layers.Dense(units=7*7*dec_in_channels, activation=lrelu),\n",
    "            tf.keras.layers.Reshape([7, 7, dec_in_channels]),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=64, kernel_size=4, strides=2, padding='same', activation=None),\n",
    "            #tf.keras.layers.Dropout(rate=drop_rate),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=64, kernel_size=4, strides=1, padding='same', activation=None),\n",
    "            #tf.keras.layers.Dropout(rate=drop_rate),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation('relu'),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=64, kernel_size=4, strides=1, padding='same', activation=lrelu),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(28*28, activation=tf.nn.sigmoid),\n",
    "            tf.keras.layers.Reshape([28,28,1])\n",
    "        ])\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.generative_net(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "        axis=raxis)\n",
    "\n",
    "def compute_loss(model, x):\n",
    "    mean, logvar = model.encode(x)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_logit = model.decode(z)\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "@tf.function\n",
    "def compute_apply_gradients(model, x, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "latent_dim = 50\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# keeping the random vector constant for generation (prediction) so\n",
    "# it will be easier to see the improvement.\n",
    "random_vector_for_generation = tf.random.normal(\n",
    "    shape=[num_examples_to_generate, latent_dim])\n",
    "model = CVAE(latent_dim, drop_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    predictions = model.sample(test_input)\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    # tight_layout minimizes the overlap between 2 sub-plots\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Test set ELBO: -534.1044311523438, time elapse for current epoch 8.284930944442749\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_and_save_images(model, 0, random_vector_for_generation)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start_time = time.time()\n",
    "    for train_x in train_dataset:\n",
    "        compute_apply_gradients(model, train_x, optimizer)\n",
    "        end_time = time.time()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        loss = tf.keras.metrics.Mean()\n",
    "    for test_x in test_dataset:\n",
    "        loss(compute_loss(model, test_x))\n",
    "    elbo = -loss.result()\n",
    "    display.clear_output(wait=False)\n",
    "    print('Epoch: {}, Test set ELBO: {}, '\n",
    "    'time elapse for current epoch {}'.format(epoch, elbo, end_time - start_time))\n",
    "    generate_and_save_images(\n",
    "    model, epoch, random_vector_for_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = CVAE(latent_dim, drop_rate=0.2)\n",
    "model.inference_net(np.zeros([1,28,28], dtype=np.float32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def encoder(X, keep_prob, n_latent):\n",
    "    activation = lrelu\n",
    "    n_X = tf.shape(X)[0]\n",
    "    with tf.name_scope(\"encoder\"):\n",
    "        x = tf.reshape(X, shape=[n_X, 28, 28, 1])\n",
    "        x = tf.keras.layers.Conv2D(name='1', filters=64, kernel_size=4, strides=2, padding='same', activation=activation)(x)\n",
    "        x = tf.keras.layers.Dropout(keep_prob, name='2')(x)\n",
    "        x = tf.keras.layers.Conv2D(name='3', filters=64, kernel_size=4, strides=2, padding='same', activation=activation)(x)\n",
    "        x = tf.keras.layers.Dropout(keep_prob, name='4')(x)\n",
    "        x = tf.keras.layers.Conv2D(name='5', filters=64, kernel_size=4, strides=1, padding='same', activation=activation)(x)\n",
    "        x = tf.keras.layers.Dropout(keep_prob, name='6')(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        \n",
    "        mu_Z = tf.keras.layers.Dense(name='7', units=n_latent)(x)\n",
    "        mu_Z = tf.reshape(mu_Z, [n_X, 1, n_latent])\n",
    "        sigma_Z = tf.math.softplus(0.5 * tf.keras.layers.Dense(name='8', units=n_latent)(x) )\n",
    "        sigma_Z = tf.reshape(sigma_Z, [n_X, 1, n_latent])\n",
    "        return mu_Z, sigma_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def encoder(X, keep_prob):\n",
    "    activation = lrelu\n",
    "    n_X = tf.shape(X)[0]\n",
    "    with tf.name_scope(\"encoder\"):\n",
    "        x = tf.reshape(X, shape=[-1, 28, 28, 1])\n",
    "        x = tf.layers.conv2d(x, name='1', filters=64, kernel_size=4, strides=2, padding='same', activation=activation)\n",
    "        x = tf.nn.dropout(x, keep_prob, name='2')\n",
    "        x = tf.layers.conv2d(x, name='3', filters=64, kernel_size=4, strides=2, padding='same', activation=activation)\n",
    "        x = tf.nn.dropout(x, keep_prob, name='4')\n",
    "        x = tf.layers.conv2d(x, name='5', filters=64, kernel_size=4, strides=1, padding='same', activation=activation)\n",
    "        x = tf.nn.dropout(x, keep_prob, name='6')\n",
    "        x = tf.contrib.layers.flatten(x)\n",
    "        \n",
    "        mu_Z = tf.layers.dense(x, name='7', units=n_latent)\n",
    "        mu_Z = tf.reshape(mu_Z, [n_X, 1, n_latent])\n",
    "        sigma_Z = tf.exp(0.5 * tf.layers.dense(x, name='8', units=n_latent) )\n",
    "        sigma_Z = tf.reshape(sigma_Z, [n_X, 1, n_latent])\n",
    "        return mu_Z, sigma_Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@tf.function\n",
    "def decoder(Z, keep_prob):\n",
    "    activation = lrelu\n",
    "    n_X = tf.shape(Z)[0]\n",
    "    n_MC = tf.shape(Z)[1]\n",
    "    with tf.name_scope(\"decoder\", reuse=tf.AUTO_REUSE):\n",
    "        x = tf.keras.layers.Dense(name='1', units=7*7*dec_in_channels//2, activation=activation)(Z)\n",
    "        x = tf.keras.layers.Dense(name='2' ,units=7*7*dec_in_channels, activation=activation)(x)\n",
    "        x = tf.reshape(x, [n_X*n_MC, 7, 7, dec_in_channels])\n",
    "        x = tf.keras.layers.Convolution2DTranspose(x, name='3', filters=64, kernel_size=4, strides=2, padding='same', activation=activation)(x)\n",
    "        x = tf.nn.dropout(x, keep_prob, name='4')(x)\n",
    "        x = tf.keras.layers.Convolution2DTranspose(x, name='5', filters=64, kernel_size=4, strides=1, padding='same', activation=activation)(x)\n",
    "        x = tf.nn.dropout(x, keep_prob, name='6')(x)\n",
    "        x = tf.keras.layers.Convolution2DTranspose(x, name='7', filters=64, kernel_size=4, strides=1, padding='same', activation=activation)(x)\n",
    "        x = tf.reshape(x, [n_X, n_MC, 14*14*64*dec_in_channels])\n",
    "        x = tf.keras.layers.Dense(x, name='8', units=28*28, activation=tf.nn.sigmoid)\n",
    "        img = tf.reshape(x, shape=[n_X, n_MC, 28, 28])\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(Z, keep_prob):\n",
    "    activation = lrelu\n",
    "    n_X = tf.shape(Z)[0]\n",
    "    n_MC = tf.shape(Z)[1]\n",
    "    with tf.variable_scope(\"decoder\", reuse=tf.AUTO_REUSE):\n",
    "        x = tf.layers.dense(Z, name='1', units=7*7*dec_in_channels//2, activation=activation)\n",
    "        x = tf.layers.dense(x, name='2' ,units=7*7*dec_in_channels, activation=lrelu)\n",
    "        x = tf.reshape(x, [n_X*n_MC, 7, 7, dec_in_channels])\n",
    "        x = tf.layers.conv2d_transpose(x, name='3', filters=64, kernel_size=4, strides=2, padding='same', activation=tf.nn.relu)\n",
    "        x = tf.nn.dropout(x, keep_prob, name='4')\n",
    "        x = tf.layers.conv2d_transpose(x, name='5', filters=64, kernel_size=4, strides=1, padding='same', activation=tf.nn.relu)\n",
    "        x = tf.nn.dropout(x, keep_prob, name='6')\n",
    "        x = tf.layers.conv2d_transpose(x, name='7', filters=64, kernel_size=4, strides=1, padding='same', activation=tf.nn.relu)\n",
    "        x = tf.reshape(x, [n_X, n_MC, 14*14*64*dec_in_channels])\n",
    "        x = tf.layers.dense(x, name='8', units=28*28, activation=tf.nn.sigmoid)\n",
    "        img = tf.reshape(x, shape=[n_X, n_MC, 28, 28])\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in train_dataset:\n",
    "    print(d.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder(d, 0.8, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO(X, Z, mu_Z, sigma_Z): \n",
    "    \n",
    "    n_latent = tf.shape(Z)[2]\n",
    "    \n",
    "    quad_pz_ary = tf.reduce_sum( -1./2. * Z**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_qz_ary = tf.reduce_sum( +1./(2. * sigma_Z**2) * (Z - mu_Z)**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_px_ary = tf.reduce_sum( -1./(2. * sigma_X**2) * (mu_X - X)**2, axis=[2,3]) #shape: [batch_size, n_MC]\n",
    "\n",
    "    #MC est. of E_q [logp(X|Z)] w.r.t. X, Z\n",
    "    img_loss = - (- (1/2.)* 28*28 *       tf.log(2 * math.pi * sigma_X**2)          + quad_px_ary \\\n",
    "                  + (1/2.)*tf.reduce_sum( tf.log(2 * math.pi * sigma_Z**2), axis=2) + quad_qz_ary)\n",
    "    img_loss = tf.reduce_mean(img_loss)#squeeze dim of n_MC\n",
    "\n",
    "    #E_q [P(Z) - logq(Z|X)] analytical value, \n",
    "    #it is a negative KL divergence (not between p(Z|X) and q(Z|X))\n",
    "    latent_loss = - (- (1/2.)*tf_float(n_latent)*np.log(2 * math.pi) + quad_pz_ary) \n",
    "    latent_loss = tf.reduce_mean(latent_loss) #squeeze dim of n_MC\n",
    "\n",
    "    return -(img_loss+latent_loss), img_loss, latent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO(mu_X, X, Z, mu_Z, sigma_Z): \n",
    "    \n",
    "    n_latent = tf.shape(Z)[2]\n",
    "    \n",
    "    quad_pz_ary = tf.reduce_sum( -1./2. * Z**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_qz_ary = tf.reduce_sum( +1./(2. * sigma_Z**2) * (Z - mu_Z)**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_px_ary = tf.reduce_sum( -1./(2. * sigma_X**2) * (mu_X - X)**2, axis=[2,3]) #shape: [batch_size, n_MC]\n",
    "\n",
    "    #MC est. of E_q [logp(X|Z)] w.r.t. X, Z\n",
    "    img_loss = - (- (1/2.)* 28*28 *       tf.log(2 * math.pi * sigma_X**2)          + quad_px_ary \\\n",
    "                  + (1/2.)*tf.reduce_sum( tf.log(2 * math.pi * sigma_Z**2), axis=2) + quad_qz_ary)\n",
    "    img_loss = tf.reduce_mean(img_loss)#squeeze dim of n_MC\n",
    "\n",
    "    #E_q [P(Z) - logq(Z|X)] analytical value, \n",
    "    #it is a negative KL divergence (not between p(Z|X) and q(Z|X))\n",
    "    latent_loss = - (- (1/2.)*tf_float(n_latent)*np.log(2 * math.pi) + quad_pz_ary) \n",
    "    latent_loss = tf.reduce_mean(latent_loss) #squeeze dim of n_MC\n",
    "\n",
    "    return -(img_loss+latent_loss), img_loss, latent_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IWAE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_IWELBO(mu_X, X, Z, mu_Z, sigma_Z):    \n",
    "    \n",
    "    #Importance MC est. of E_p[ \"exp. part of P(X|z)\" ] given X w.r.t. Z\n",
    "    #for numerical stability, a little complicated\n",
    "    \n",
    "    n_X = tf.shape(X)[0]\n",
    "    n_latent = tf.shape(mu_Z)[2]\n",
    "    \n",
    "    quad_pz_ary = tf.reduce_sum( 1./2. * Z**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_qz_ary = tf.reduce_sum( 1./(2. * sigma_Z**2) * (Z - mu_Z)**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_px_ary = tf.reduce_sum( 1./(2. * sigma_X**2) * (X - mu_X)**2, axis=[2,3]) #shape: [batch_size, n_MC]\n",
    "    \n",
    "    quad_ary = - quad_pz_ary + quad_qz_ary - quad_px_ary\n",
    "    \n",
    "    IWELBO = (1/2.) * tf.reduce_sum( tf.reshape(tf.log(sigma_Z**2), [n_X, n_latent]), axis=1)\\\n",
    "           - (1/2.)*28*28 * tf.log(2 * math.pi * sigma_X**2)\\\n",
    "           + tf_reduce_logmeanexp(quad_ary, axis=1)\n",
    "\n",
    "    return IWELBO #shape: [batch_size] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_gamma_div(mu_X, X, Z, mu_Z, sigma_Z, gamma):    \n",
    "    \n",
    "    #Importance MC est. of E_p[ \"exp. part of P(X|z)\" ] given X w.r.t. Z\n",
    "    #for numerical stability, a little complicated\n",
    "    \n",
    "    n_X = tf.shape(X)[0]\n",
    "    n_latent = tf.shape(Z)[2]\n",
    "\n",
    "    quad_pz_ary = tf.reduce_sum( 1./2. * Z**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_qz_ary = tf.reduce_sum( 1./(2. * sigma_Z**2) * (Z - mu_Z)**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_px_ary = tf.reduce_sum( 1./(2. * sigma_X**2) * (X - mu_X)**2, axis=[2,3]) #shape: [batch_size, n_MC]\n",
    "    \n",
    "    quad_ary = - quad_pz_ary + quad_qz_ary - quad_px_ary\n",
    "    \n",
    "    gamma_div = (1/2.) * tf.reduce_sum( tf.reshape(tf.log(sigma_Z**2), [n_X, n_latent]), axis=1)\\\n",
    "           - (1/2.)*28*28 * tf.log(2 * math.pi * sigma_X**2)\\\n",
    "           + (1/gamma) * tf_reduce_logmeanexp(gamma * quad_ary, axis=1)\n",
    "\n",
    "    return gamma_div #shape: [batch_size] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointwise_Pearson_UBO = lambda mu_X, X, Z, mu_Z, sigma_Z: pointwise_gamma_div(mu_X, X, Z, mu_Z, sigma_Z, gamma=2.)\n",
    "pointwise_Hellinger_LBO = lambda mu_X, X, Z, mu_Z, sigma_Z: pointwise_gamma_div(mu_X, X, Z, mu_Z, sigma_Z, gamma=0.5)\n",
    "pointwise_Neyman_LBO = lambda mu_X, X, Z, mu_Z, sigma_Z: pointwise_gamma_div(mu_X, X, Z, mu_Z, sigma_Z, gamma=-1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_reversed_gamma_div(mu_X, X, Z, mu_Z, sigma_Z, gamma):    \n",
    "    \n",
    "    #Importance MC est. of E_p[ \"exp. part of P(X|z)\" ] given X w.r.t. Z\n",
    "    #for numerical stability, a little complicated\n",
    "    \n",
    "    n_X = tf.shape(X)[0]\n",
    "    n_latent = tf.shape(Z)[2]\n",
    "\n",
    "    quad_pz_ary = tf.reduce_sum( 1./2. * Z**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_qz_ary = tf.reduce_sum( 1./(2. * sigma_Z**2) * (Z - mu_Z)**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_px_ary = tf.reduce_sum( 1./(2. * sigma_X**2) * (X - mu_X)**2, axis=[2,3]) #shape: [batch_size, n_MC]\n",
    "    \n",
    "    quad_ary = - quad_pz_ary + quad_qz_ary - quad_px_ary\n",
    "    \n",
    "    first_term = -((gamma-1)/2.) * tf.reduce_sum( tf.reshape(tf.log(sigma_Z**2), [n_X, n_latent]), axis=1)\\\n",
    "           + ((gamma-1)/2.)*28*28 * tf.log(2 * math.pi * sigma_X**2)\\\n",
    "           + tf_reduce_logmeanexp( - (gamma-1) * quad_ary, axis=1)\n",
    "    first_term *= - (1/gamma)\n",
    "    \n",
    "    second_term = (1/2.) * tf.reduce_sum( tf.reshape(tf.log(sigma_Z**2), [n_X, n_latent]), axis=1)\\\n",
    "           - (1/2.)*28*28 * tf.log(2 * math.pi * sigma_X**2)\\\n",
    "           + tf_reduce_logmeanexp(quad_ary, axis=1)\n",
    "    second_term *= (1/gamma)\n",
    "\n",
    "    return first_term + second_term #shape: [batch_size] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointwise_Pearson_LBO = lambda mu_X, X, Z, mu_Z, sigma_Z: pointwise_reversed_gamma_div(mu_X, X, Z, mu_Z, sigma_Z, gamma=2.)\n",
    "pointwise_Hellinger_UBO = lambda mu_X, X, Z, mu_Z, sigma_Z: pointwise_reversed_gamma_div(mu_X, X, Z, mu_Z, sigma_Z, gamma=0.5)\n",
    "pointwise_Neyman_UBO = lambda mu_X, X, Z, mu_Z, sigma_Z: pointwise_reversed_gamma_div(mu_X, X, Z, mu_Z, sigma_Z, gamma=-1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_reversed_KL_UBO(mu_X, X, Z, mu_Z, sigma_Z):    \n",
    "    \n",
    "    #Importance MC est. of E_p[ \"exp. part of P(X|z)\" ] given X w.r.t. Z\n",
    "    #for numerical stability, a little complicated\n",
    "    \n",
    "    n_X = tf.shape(X)[0]\n",
    "    n_latent = tf.shape(Z)[2]\n",
    "\n",
    "    quad_pz_ary = tf.reduce_sum( 1./2. * Z**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_qz_ary = tf.reduce_sum( 1./(2. * sigma_Z**2) * (Z - mu_Z)**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_px_ary = tf.reduce_sum( 1./(2. * sigma_X**2) * (X - mu_X)**2, axis=[2,3]) #shape: [batch_size, n_MC]\n",
    "    \n",
    "    quad_ary = - quad_pz_ary + quad_qz_ary - quad_px_ary\n",
    "    \n",
    "    log_w = (1/2.) * tf.reduce_sum( tf.reshape(tf.log(sigma_Z**2), [n_X, n_latent,1]), axis=1)\\\n",
    "           - (1/2.)*28*28 * tf.log(2 * math.pi * sigma_X**2)\\\n",
    "           + quad_ary\n",
    "    reversed_KL = tf.nn.softmax(log_w, axis=1)*log_w #shape: [batch_size, n_MC]\n",
    "    reversed_KL = tf.reduce_sum(reversed_KL, axis=1)\n",
    "    \n",
    "    return reversed_KL #shape: [batch_size] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_mutual_information(mu_X, X, Z, mu_Z, sigma_Z):    \n",
    "    \n",
    "    #Importance MC est. of E_p[ \"exp. part of P(X|z)\" ] given X w.r.t. Z\n",
    "    #for numerical stability, a little complicated\n",
    "    \n",
    "    n_X = tf.shape(X)[0]\n",
    "    n_latent = tf.shape(Z)[2]\n",
    "\n",
    "    quad_pz_ary = tf.reduce_sum( 1./2. * Z**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_qz_ary = tf.reduce_sum( 1./(2. * sigma_Z**2) * (Z - mu_Z)**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_px_ary = tf.reduce_sum( 1./(2. * sigma_X**2) * (X - mu_X)**2, axis=[2,3]) #shape: [batch_size, n_MC]\n",
    "    \n",
    "    quad_ary = - quad_pz_ary + quad_qz_ary - quad_px_ary\n",
    "    \n",
    "    log_w = (1/2.) * tf.reduce_sum( tf.reshape(tf.log(sigma_Z**2), [n_X, n_latent,1]), axis=1)\\\n",
    "             - (1/2.)*28*28 * tf.log(2 * math.pi * sigma_X**2)\\\n",
    "             + quad_ary\n",
    "    log_px = - (1/2.)*28*28 * tf.log(2 * math.pi * sigma_X**2)\\\n",
    "             - quad_px_ary\n",
    "    first_term = tf.nn.softmax(log_w, axis=1)*log_px #shape: [batch_size, n_MC]\n",
    "    first_term = tf.reduce_sum(first_term, axis=1) \n",
    "    second_term = - pointwise_IWELBO(mu_X, X, Z, mu_Z, sigma_Z)\n",
    "    \n",
    "    return first_term + second_term#shape: [batch_size] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMC IWAE loss (non-adaptive, heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_psi(l, X, pointwise_nested):\n",
    "\n",
    "    n_MC = 2**l\n",
    "    n_X = tf.shape(X)[0]\n",
    "    \n",
    "    mu_Z, sigma_Z = encoder(X, keep_prob)\n",
    "    Z = Z_sampler(mu_Z, sigma_Z, n_MC)\n",
    "    mu_X = decoder(Z, keep_prob)\n",
    "    \n",
    "    nested_whole = pointwise_nested(mu_X, X, Z, mu_Z, sigma_Z)\n",
    "\n",
    "    Z_1st_half = Z[:, :n_MC//2 , :]\n",
    "    Z_2nd_half = Z[:,  n_MC//2:, :]\n",
    "    \n",
    "    mu_X_1st_half = mu_X[:, :n_MC//2 , :, :]\n",
    "    mu_X_2nd_half = mu_X[:,  n_MC//2:, :, :]\n",
    "    \n",
    "    nested_1st_half = pointwise_nested(mu_X_1st_half, X, Z_1st_half, mu_Z, sigma_Z)\n",
    "    nested_2nd_half = pointwise_nested(mu_X_2nd_half, X, Z_2nd_half, mu_Z, sigma_Z)\n",
    "    \n",
    "    #psis is a [batch_size] dimentional tensor \n",
    "    pointwise_psi = nested_whole - (1./2.) * ( nested_1st_half + nested_2nd_half )\n",
    "    return pointwise_psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLMC_est(X, pointwise_nested, start_level, max_level, n_X, verbose=False): \n",
    "\n",
    "    levels = list(range(start_level, max_level+1))\n",
    "    Ns = {}\n",
    "    Cs = {}\n",
    "    NCs = {}\n",
    "    weight = np.array([2**(-3/2*l) for l in range(max_level+1)])\n",
    "    weight[:start_level] = 0\n",
    "    \n",
    "    for l in levels:\n",
    "        Ns[l] = math.ceil( (weight[l]/weight.sum()) * n_X ) \n",
    "        Cs[l] = 2**l\n",
    "        NCs[l] = Ns[l]*Cs[l]\n",
    "    \n",
    "    Ns[start_level] = n_X - sum([Ns[l] for l in levels[1:]])\n",
    "    if verbose:\n",
    "        print( {'N':Ns, 'C':Cs, 'NC':NCs, 'total_cost':sum(NCs.values())})\n",
    "    #MC est. of - E_p[log E_p[P(X|Z_sample)] |X ] w.r.t. X, psi_sample\n",
    "    \n",
    "    mu_Z, sigma_Z = encoder(X[:Ns[start_level]], keep_prob)\n",
    "    Z = Z_sampler(mu_Z, sigma_Z, Cs[start_level])\n",
    "    mu_X = decoder(Z, keep_prob)\n",
    "    \n",
    "    obj = tf.reduce_mean( pointwise_nested(mu_X, X[:Ns[start_level]], Z, mu_Z, sigma_Z) )\n",
    "    \n",
    "    for l in levels[1:]:\n",
    "        start_idx = sum(list(Ns.values())[:l-start_level])\n",
    "        obj += tf.reduce_mean( pointwise_psi(l, X[start_idx : start_idx+Ns[l]], pointwise_IWELBO) )\n",
    "        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode, sample, encode\n",
    "X = train_images.\n",
    "mu_Z, sigma_Z = encoder(X, keep_prob)\n",
    "Z = Z_sampler(mu_Z, sigma_Z, n_MC=1)\n",
    "mu_X = decoder(Z, keep_prob)\n",
    "\n",
    "_, latent_loss, img_loss = ELBO(mu_X, X, Z, mu_Z, sigma_Z)\n",
    "loss['ELBO'] = latent_loss + img_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode, sample, encode\n",
    "X = X0[:8]\n",
    "mu_Z, sigma_Z = encoder(X, keep_prob)\n",
    "Z = Z_sampler(mu_Z, sigma_Z, n_MC=32)\n",
    "mu_X = decoder(Z, keep_prob)\n",
    "\n",
    "IWELBO = pointwise_IWELBO(mu_X, X, Z, mu_Z, sigma_Z)\n",
    "loss['IWELBO'] = - tf.reduce_mean(IWELBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss['MLMC-IWELBO'] = - MLMC_est(X0, pointwise_nested=pointwise_IWELBO, start_level=1, max_level=5, n_X=50, verbose=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconsrtruction based on importance sampling / Random Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X0[:1,:1,:,:]\n",
    "mu_Z,sigma_Z = encoder(X, keep_prob=1)\n",
    "reconstructed = decoder(mu_Z, keep_prob=1)\n",
    "\n",
    "Z = Z_sampler(tf.constant(np.zeros([1,1,n_latent],dtype=float_type)), 1, 1)\n",
    "generated = decoder(Z, keep_prob=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizing_ops(loss_p=None, loss_q=None, alpha_p=0.0005, alpha_q=0.0005):\n",
    "\n",
    "    optimizer_p = tf.train.AdamOptimizer(alpha_p, name='optimizer')\n",
    "    optimizer_q = tf.train.AdamOptimizer(alpha_q, name='optimizer')\n",
    "    \n",
    "    \n",
    "    ops = []\n",
    "    \n",
    "    if loss_p is not None:\n",
    "        ops.append(  )\n",
    "    if loss_q is not None:\n",
    "        ops.append( optimizer_q.minimize(loss_q, var_list=param_enc) )\n",
    "    return ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.0005, name='optimizer')\n",
    "\n",
    "param_dec = tf.trainable_variables(scope='decoder')\n",
    "param_enc = tf.trainable_variables(scope='encoder')\n",
    "\n",
    "op_elbo_p = optimizer.minimize(loss['ELBO'], var_list=param_dec)\n",
    "op_elbo_q = optimizer.minimize(loss['ELBO'], var_list=param_enc)\n",
    "\n",
    "ops = [op_elbo_q, op_elbo_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16,16])\n",
    "n_iter = 1000\n",
    "losses = []\n",
    "\n",
    "for i in range(n_iter):\n",
    "    batch = mnist.train.next_batch(batch_size=64)[0].reshape([-1,28,28])\n",
    "    _ = sess.run(ops, feed_dict = {X_in: batch, keep_prob: 0.8})\n",
    "        \n",
    "    if 0 == i % (n_iter//50):\n",
    "        j = i//(n_iter//50)\n",
    "        ls, r, i_ls, l_ls = sess.run([loss['ELBO'], reconstructed, img_loss, latent_loss], feed_dict = {X_in: batch, keep_prob: 1.0})\n",
    "        losses.append(ls)\n",
    "        plt.subplot(10,10,j//10*20+j%10+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(np.reshape(batch[0], [28, 28]), cmap='gray')\n",
    "        plt.title('#epoch:%0.0d'%(i))\n",
    "        plt.subplot(10,10,j//10*20+10+j%10+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(r[0,0,:,:], cmap='gray')\n",
    "        print(\"#epochs:%0.0d,\\tloss:%0.2f,\\timage_loss:%0.2f,\\tlatent_loss:%0.2f\"%(i, ls, np.mean(i_ls), np.mean(l_ls)))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLMC condition check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the variance of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(ops, n_step):\n",
    "    for i in range(n_step):\n",
    "        batch = mnist.train.next_batch(batch_size=batch_size)[0]\n",
    "        batch = batch.reshape([-1,28,28])\n",
    "        sess.run(ops, feed_dict = {X_in: batch, keep_prob: 0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dec_cost_mlmc(start_level, max_level, n_X):\n",
    "    levels = list(range(start_level, max_level+1))\n",
    "    Ns = {}\n",
    "    Cs = {}\n",
    "    NCs = {}\n",
    "    weight = np.array([2**(-3/2*l) for l in range(max_level+1)])\n",
    "    weight[:start_level] = 0\n",
    "\n",
    "    for l in levels:\n",
    "        Ns[l] = math.ceil( (weight[l]/weight.sum()) * n_X ) \n",
    "        Cs[l] = 2**l\n",
    "        NCs[l] = Ns[l]*Cs[l]\n",
    "    \n",
    "    return (sum(list(NCs.values())))\n",
    "\n",
    "def eval_mean_and_var(obj_list, n_sample):\n",
    "    mean_list = []\n",
    "    var_list = []\n",
    "    for obj in obj_list:\n",
    "        batches = [mnist.test.next_batch(batch_size=batch_size)[0].reshape([-1,28,28]) for i in range(n_sample)]\n",
    "        vals = [sess.run([obj], feed_dict = {X_in: batch, keep_prob: 1.0}) for batch in batches]\n",
    "        mean_list.append(np.mean(vals))\n",
    "        var_list.append(np.var(vals))\n",
    "    return mean_list, var_list\n",
    "\n",
    "def eval_standard_complexity(pointwise_nested, start_level, max_level, n_sample=20):\n",
    "    X = X0[:10]\n",
    "    psis = []\n",
    "    nested = []\n",
    "    mlmc = []\n",
    "    \n",
    "    dec_cost_nested = []\n",
    "    enc_cost_nested = []\n",
    "    dec_cost_mlmc = []\n",
    "    enc_cost_mlmc = []\n",
    "    \n",
    "    for l in range(start_level,max_level+1):\n",
    "        \n",
    "        mu_Z, sigma_Z = encoder(X, keep_prob)\n",
    "        Z = Z_sampler(mu_Z, sigma_Z, 2**l)\n",
    "        mu_X = decoder(Z, keep_prob)\n",
    "        \n",
    "        psis.append( tf.reduce_mean( pointwise_psi(l+1, X, pointwise_nested) ) )\n",
    "        nested.append( tf.reduce_mean( pointwise_nested(mu_X, X, Z, mu_Z, sigma_Z) ) ) \n",
    "        mlmc.append( MLMC_est(tf.reshape(X0[:100], [100, 1, 28, 28]), pointwise_nested=pointwise_IWELBO, start_level=start_level, max_level=l, n_X=100) )\n",
    "\n",
    "        dec_cost_nested.append(10)\n",
    "        enc_cost_nested.append(10*2**l)\n",
    "        dec_cost_mlmc.append(100)\n",
    "        enc_cost_mlmc.append( get_dec_cost_mlmc(start_level, l, n_X=100) )\n",
    "        \n",
    "    mean_psi, var_psi    = eval_mean_and_var(psis, n_sample)\n",
    "    _,        var_nested = eval_mean_and_var(nested, n_sample)\n",
    "    _,        var_mlmc   = eval_mean_and_var(mlmc, n_sample)\n",
    "    \n",
    "    cost_nested = np.array(enc_cost_nested) * (np.array(var_nested) / (4 * np.array(var_psi + np.array(mean_psi)**2)))\n",
    "    cost_mlmc   = np.array(enc_cost_mlmc  ) * (np.array(var_mlmc  ) / (4 * np.array(var_psi + np.array(mean_psi)**2)))\n",
    "\n",
    "    #print(var_nested)\n",
    "    #print(var_mlmc)\n",
    "    #print(var_psi)\n",
    "    \n",
    "    #print(enc_cost_nested)\n",
    "    #print(enc_cost_mlmc)\n",
    "    \n",
    "    return np.array([4 * np.sqrt(var_psi + np.array(mean_psi)**2), cost_nested, cost_mlmc]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_cost_accuracy(pointwise_nested, obj_name, start_level=0, max_level=7, n_sample=100):\n",
    "    r = eval_standard_complexity(pointwise_nested, start_level, max_level, n_sample)\n",
    "    x = r[:,0]/max(r[:,0])\n",
    "    x0 = x[0]\n",
    "    y = r[:,[1,2]]\n",
    "    y0 = y[0,:] = np.mean(y[0,:])\n",
    "    plt.plot(x,y)\n",
    "    plt.plot(x0*2.**(-np.arange(len(x))), y0*2.**(2*np.arange(len(y))), color='gray')\n",
    "    plt.plot(x0*2.**(-np.arange(len(x))), y0*2.**(3*np.arange(len(y))), color='gray')\n",
    "    plt.xlim([max(x)*1.1, min(x)*0.9])\n",
    "    plt.ylim([min(y[:,0])*0.5, max(y[:,0])*2])\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('relative accuracy: epsilon')\n",
    "    plt.ylabel('required cost')\n",
    "    plt.legend(['cost of nested estimator', 'cost of MLMC estimator', 'O(1/epsilon^2)', 'O(1/epsilon^3)'])\n",
    "    plt.savefig('../out/plot/cost_accuracy_{}.eps'.format(obj_name))\n",
    "    save(r, '../out/log/cost_accuracy_{}.pickle'.format(obj_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check of the order of psi (difference of estimators between levels) and IWELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergenvce(pointwise_nested, start_level, max_level, n_sample=20, figname=None):\n",
    "    \n",
    "    levels = np.arange(start_level,max_level+1)\n",
    "    psis = []\n",
    "    \n",
    "    for l in levels:\n",
    "        psis.append( tf.reduce_mean( pointwise_psi(l, X0[:10], pointwise_nested) ) )\n",
    "    \n",
    "    mean_psi, var_psi = eval_mean_and_var(psis, n_sample)\n",
    "    var_psi = np.array(var_psi)*10\n",
    "    mean_abs_psi = np.abs(mean_psi)\n",
    "    \n",
    "    plt.figure(figsize=[8,6])\n",
    "    plt.plot(levels,var_psi)\n",
    "    plt.plot(levels,mean_abs_psi)\n",
    "    plt.plot(levels,[var_psi[0]*2.**(-2*l) for l in range(len(levels))],c='gray')\n",
    "    plt.plot(levels,[mean_abs_psi[0]*2.**(-l) for l in range(len(levels))],c='gray')\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('variance')\n",
    "    plt.xlabel('level')\n",
    "    plt.legend([\n",
    "        'Var_X[ psi_l(X) ]\\n',\n",
    "        '|E_X[ psi_l(X) ]|\\n',\n",
    "        'O(2^(-2l))', 'O(2^(-l))'\n",
    "    ])\n",
    "    if figname is not None:\n",
    "        plt.savefig('../out/plot/psi_convergence_{}.eps'.format(figname))\n",
    "        save(np.array([var_psi, mean_abs_psi]), '../out/log/psi_convergence_{}.pickle'.format(figname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the variance of gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mean_and_var_grad(obj, n_sample):\n",
    "    \n",
    "    mean_list = []\n",
    "    var_list = []\n",
    "    \n",
    "    g = tf.train.GradientDescentOptimizer(0.0005)\n",
    "    grads = g.compute_gradients(obj)\n",
    "    grads_flat = tf.concat([tf.reshape(g_tuple[0],[-1]) for g_tuple in grads], axis=0)\n",
    "    \n",
    "    mean = 0\n",
    "    mean_sq = 0\n",
    "    for i in range(n_sample):\n",
    "        batch = mnist.test.next_batch(batch_size=batch_size)[0].reshape([-1,28,28])\n",
    "        tmp_grads = np.float64(sess.run(grads_flat, feed_dict = {X_in: batch, keep_prob: 1.0}))\n",
    "        mean    += (1/n_sample) * np.abs(tmp_grads)\n",
    "        mean_sq += (1/n_sample) * tmp_grads**2\n",
    "    var = mean_sq - mean**2\n",
    "    \n",
    "    return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergenvce_grad(pointwise_nested, start_level, max_level, n_sample=20, figname=None):\n",
    "    \n",
    "    levels = np.arange(start_level,max_level+1)\n",
    "    moment2 = []\n",
    "    moment1 = []\n",
    "    \n",
    "    for l in levels:\n",
    "        psi = tf.reduce_mean( pointwise_psi(l, X0[:10], pointwise_nested) )\n",
    "        mean_psi_grad, var_psi_grad = eval_mean_and_var_grad(psi, n_sample)\n",
    "        moment2.append( sum(var_psi_grad)*10 )\n",
    "        moment1.append( sum(np.abs(mean_psi_grad)) )\n",
    "\n",
    "    plt.figure(figsize=[8,6])\n",
    "    plt.plot(levels, moment2)\n",
    "    plt.plot(levels, moment1)\n",
    "    plt.plot(levels,[moment2[0]*2.**(-2*l) for l in range(len(levels))],c='gray')\n",
    "    plt.plot(levels,[moment1[0]*2.**(-l) for l in range(len(levels))],c='gray')\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('variance')\n",
    "    plt.xlabel('level')\n",
    "    plt.legend([\n",
    "        '||Var_X[ nabla psi_l(X) ]||_tr',\n",
    "        '||E_X[ nabla psi_l(X) ]||_1',\n",
    "        'O(2^(-2l))', 'O(2^(-l))'\n",
    "    ])\n",
    "    if figname is not None:\n",
    "        plt.savefig('../out/plot/psi_grad_convergence_{}.eps'.format(figname))\n",
    "        save(np.array([moment1, moment2]), '../out/log/psi_grad_convergence_{}.pickle'.format(figname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'iwelbo':pointwise_IWELBO,\n",
    "    'reversed_KL':pointwise_reversed_KL_UBO,\n",
    "    'mutual_info':pointwise_mutual_information,\n",
    "    'hellinger_lbo':pointwise_Hellinger_LBO,\n",
    "    'hellinger_ubo':pointwise_Hellinger_UBO,\n",
    "    'neyman_lbo':pointwise_Neyman_LBO,\n",
    "    'neyman_ubo':pointwise_Neyman_UBO,\n",
    "    'pearson_lbo':pointwise_Pearson_LBO,\n",
    "    'pearson_ubo':pointwise_Pearson_UBO\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "train_model(ops, n_step=200)\n",
    "\n",
    "for obj_name, pointwise_nested in metrics.items():\n",
    "    plot_cost_accuracy(pointwise_nested, '{}_{}_step_temp'.format(obj_name, 200), start_level=0, max_level=7, n_sample=100)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "#train_model(ops, n_step=200)\n",
    "\n",
    "for obj_name, pointwise_nested in metrics.items():\n",
    "    #plot_cost_accuracy(pointwise_nested, '{}_{}_step'.format(obj_name, 200), start_level=0, max_level=7, n_sample=100)\n",
    "    print('completed: {}_{}_step'.format(obj_name, 200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [200, 300, 400, 500, 1000, 1500]\n",
    "delta_steps = np.array(steps[1:]) - np.array(steps[:-1])\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "if steps[0]!=0:\n",
    "    train_model(ops, n_step=steps[0])\n",
    "    \n",
    "for obj_name, pointwise_nested in metrics.items():\n",
    "    plot_convergenvce(pointwise_nested, start_level=1, max_level=7, n_sample=100, \n",
    "                      figname='{}_{}_step'.format(obj_name, steps[0]))\n",
    "    plot_convergenvce_grad(pointwise_nested, start_level=1, max_level=7, n_sample=100, \n",
    "                      figname='{}_{}_step'.format(obj_name, steps[0]))\n",
    "    print('completed: {}_{}_step'.format(obj_name, steps[0]))\n",
    "\n",
    "for i in range(len(delta_steps)):\n",
    "    train_model(ops, n_step=delta_steps[i])\n",
    "    for obj_name, pointwise_nested in metrics.items():\n",
    "        plot_convergenvce(pointwise_nested, start_level=1, max_level=7, n_sample=100, \n",
    "                          figname='{}_{}_step'.format(obj_name, steps[i+1]))\n",
    "        plot_convergenvce_grad(pointwise_nested, start_level=1, max_level=7, n_sample=100, \n",
    "                          figname='{}_{}_step'.format(obj_name, steps[i+1]))\n",
    "        print('completed: {}_{}_step'.format(obj_name, steps[i+1]))\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of IWELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_psi_convergence_plots(pointwise_nested, steps, obj_name, train_ops):\n",
    "    delta_steps = np.array(steps[1:]) - np.array(steps[:-1])\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    if steps[0]!=0:\n",
    "        train_model(train_ops, n_step=steps[0])\n",
    "    plot_convergenvce(pointwise_nested, start_level=1, max_level=9, n_sample=100, \n",
    "                      figname='../out/{}_psi_convergence_{}_step.eps'.format(obj_name,steps[0]))\n",
    "    \n",
    "    for i in range(len(delta_steps)):\n",
    "        train_model(train_ops, n_step=delta_steps[i])\n",
    "        plot_convergenvce(pointwise_nested, start_level=1, max_level=9, n_sample=100, \n",
    "                      figname='../out/{}_psi_convergence_{}_step.eps'.format(obj_name, steps[i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_psi_grad_convergence_plots(pointwise_nested, steps, obj_name, train_ops):\n",
    "    delta_steps = np.array(steps[1:]) - np.array(steps[:-1])\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    if steps[0]!=0:\n",
    "        train_model(train_ops, n_step=steps[0])\n",
    "    plot_convergenvce_grad(pointwise_nested, start_level=1, max_level=9, n_sample=100, \n",
    "                      figname='../out/psi_grad_convergence_{}_{}_step.eps'.format(obj_name,steps[0]))\n",
    "    \n",
    "    for i in range(len(delta_steps)):\n",
    "        train_model(train_ops, n_step=delta_steps[i])\n",
    "        plot_convergenvce_grad(pointwise_nested, start_level=1, max_level=9, n_sample=100, \n",
    "                      figname='../out/{}_psi_grad_convergence_{}_step.eps'.format(obj_name,steps[i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_psi_convergence_plots(pointwise_IWELBO, steps=[0,100,200,300,400,500,1000,1500], obj_name='iwelbo', train_ops=ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_psi_grad_convergence_plots(pointwise_IWELBO, steps=[0,100,200,300,400,500,1000,1500], obj_name='iwelbo', train_ops=ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cost_accuracy(pointwise_IWELBO, 'iwelbo', start_level=0, max_level=7, n_sample=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "get_psi_convergence_plots(pointwise_IWELBO, steps=[0,100,200,300,400,500,1000,1500], obj_name='iwelbo', train_ops=ops)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
