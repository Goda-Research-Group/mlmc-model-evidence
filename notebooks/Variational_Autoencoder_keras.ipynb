{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・勾配で、計算量のプロットを作る\n",
    "・global parameter（n_latent, sess, mnistなど）を消す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "#import scipy.stats\n",
    "#import math\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "# Normalizing the images to the range of [0., 1.]\n",
    "train_images /= 255.\n",
    "test_images /= 255.\n",
    "\n",
    "# Binarization\n",
    "train_images[train_images >= .5] = 1.\n",
    "train_images[train_images < .5] = 0.\n",
    "test_images[test_images >= .5] = 1.\n",
    "test_images[test_images < .5] = 0.\n",
    "\n",
    "TRAIN_BUF = 60000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "TEST_BUF = 10000\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(TRAIN_BUF).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_images).shuffle(TEST_BUF).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_in = tf.placeholder(dtype=float_type, shape=[None, 28, 28], name='X')\n",
    "#X0 = tf.reshape(X_in,[-1, 1, 28, 28])\n",
    "drop_rate = 0.2\n",
    "#with tf.name_scope(\"decoder\"):\n",
    "#    n_beta = tf.Variable(1., 'n_beta', dtype=float_type) #variance of image output\n",
    "\n",
    "n_latent = 5\n",
    "dec_in_channels = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# hyper paremeters depending on models\n",
    "batch_size = 100\n",
    "n_MC = {}\n",
    "loss = {}\n",
    "optimizer = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility funcs\n",
    "float_type = np.float32\n",
    "\n",
    "tf_int32 = lambda x:tf.cast(x, tf.int32)\n",
    "tf_float = lambda x:tf.cast(x, float_type)\n",
    "\n",
    "def lrelu(x, alpha=0.3):\n",
    "    return tf.maximum(x, tf.multiply(x, alpha))\n",
    "\n",
    "def tf_reduce_logmeanexp(ary, axis=1):\n",
    "    n_MC = tf.shape(ary)[axis]\n",
    "    return tf.reduce_logsumexp(ary, axis=axis) - tf.log(tf_float(n_MC))\n",
    "\n",
    "def tf_sum(_list):\n",
    "    S = 0\n",
    "    for val in _list:\n",
    "        S += val\n",
    "    return val\n",
    "\n",
    "def save(data_obj, name):\n",
    "    with open(name, mode='wb') as f:\n",
    "        pickle.dump(data_obj, f)\n",
    "        \n",
    "def load(name):\n",
    "    with open(name, mode='rb') as f:\n",
    "        data_obj = pickle.load(f)\n",
    "    return data_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, latent_dim, drop_rate):\n",
    "        \n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.drop_rate = drop_rate\n",
    "        \n",
    "        self.inference_net = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=4, strides=2, padding='same', activation=lrelu),\n",
    "            #tf.keras.layers.Dropout(rate=drop_rate),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=4, strides=2, padding='same', activation=lrelu),\n",
    "            #tf.keras.layers.Dropout(rate=drop_rate),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=4, strides=1, padding='same', activation=lrelu),\n",
    "            #tf.keras.layers.Dropout(rate=drop_rate),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "        ])\n",
    "        \n",
    "        self.generative_net = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=(latent_dim)),\n",
    "            tf.keras.layers.Dense(units=7*7*dec_in_channels//2, activation=lrelu),\n",
    "            tf.keras.layers.Dense(units=7*7*dec_in_channels, activation=lrelu),\n",
    "            tf.keras.layers.Reshape([7, 7, dec_in_channels]),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=64, kernel_size=4, strides=2, padding='same', activation=lrelu),\n",
    "            #tf.keras.layers.Dropout(rate=drop_rate),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=64, kernel_size=4, strides=1, padding='same', activation=lrelu),\n",
    "            #tf.keras.layers.Dropout(rate=drop_rate),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=64, kernel_size=4, strides=1, padding='same', activation=lrelu),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(28*28, activation=tf.nn.sigmoid),\n",
    "            tf.keras.layers.Reshape([28,28,1])\n",
    "        ])\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.generative_net(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "        axis=raxis)\n",
    "\n",
    "def compute_loss(model, x):\n",
    "    mean, logvar = model.encode(x)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_logit = model.decode(z)\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "@tf.function\n",
    "def compute_apply_gradients(model, x, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "latent_dim = 50\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# keeping the random vector constant for generation (prediction) so\n",
    "# it will be easier to see the improvement.\n",
    "random_vector_for_generation = tf.random.normal(\n",
    "    shape=[num_examples_to_generate, latent_dim])\n",
    "model = CVAE(latent_dim, drop_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    predictions = model.sample(test_input)\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    # tight_layout minimizes the overlap between 2 sub-plots\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Test set ELBO: -543.3816528320312, time elapse for current epoch 9.033736944198608\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_and_save_images(model, 0, random_vector_for_generation)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    start_time = time.time()\n",
    "    for train_x in train_dataset:\n",
    "        compute_apply_gradients(model, train_x, optimizer)\n",
    "        end_time = time.time()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        loss = tf.keras.metrics.Mean()\n",
    "    for test_x in test_dataset:\n",
    "        loss(compute_loss(model, test_x))\n",
    "    elbo = -loss.result()\n",
    "    display.clear_output(wait=False)\n",
    "    print('Epoch: {}, Test set ELBO: {}, '\n",
    "    'time elapse for current epoch {}'.format(epoch, elbo, end_time - start_time))\n",
    "    generate_and_save_images(\n",
    "    model, epoch, random_vector_for_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = CVAE(latent_dim, drop_rate=0.2)\n",
    "model.inference_net(np.zeros([1,28,28], dtype=np.float32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def encoder(X, keep_prob, n_latent):\n",
    "    activation = lrelu\n",
    "    n_X = tf.shape(X)[0]\n",
    "    with tf.name_scope(\"encoder\"):\n",
    "        x = tf.reshape(X, shape=[n_X, 28, 28, 1])\n",
    "        x = tf.keras.layers.Conv2D(name='1', filters=64, kernel_size=4, strides=2, padding='same', activation=activation)(x)\n",
    "        x = tf.keras.layers.Dropout(keep_prob, name='2')(x)\n",
    "        x = tf.keras.layers.Conv2D(name='3', filters=64, kernel_size=4, strides=2, padding='same', activation=activation)(x)\n",
    "        x = tf.keras.layers.Dropout(keep_prob, name='4')(x)\n",
    "        x = tf.keras.layers.Conv2D(name='5', filters=64, kernel_size=4, strides=1, padding='same', activation=activation)(x)\n",
    "        x = tf.keras.layers.Dropout(keep_prob, name='6')(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        \n",
    "        mu_Z = tf.keras.layers.Dense(name='7', units=n_latent)(x)\n",
    "        mu_Z = tf.reshape(mu_Z, [n_X, 1, n_latent])\n",
    "        sigma_Z = tf.math.softplus(0.5 * tf.keras.layers.Dense(name='8', units=n_latent)(x) )\n",
    "        sigma_Z = tf.reshape(sigma_Z, [n_X, 1, n_latent])\n",
    "        return mu_Z, sigma_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def encoder(X, keep_prob):\n",
    "    activation = lrelu\n",
    "    n_X = tf.shape(X)[0]\n",
    "    with tf.name_scope(\"encoder\"):\n",
    "        x = tf.reshape(X, shape=[-1, 28, 28, 1])\n",
    "        x = tf.layers.conv2d(x, name='1', filters=64, kernel_size=4, strides=2, padding='same', activation=activation)\n",
    "        x = tf.nn.dropout(x, keep_prob, name='2')\n",
    "        x = tf.layers.conv2d(x, name='3', filters=64, kernel_size=4, strides=2, padding='same', activation=activation)\n",
    "        x = tf.nn.dropout(x, keep_prob, name='4')\n",
    "        x = tf.layers.conv2d(x, name='5', filters=64, kernel_size=4, strides=1, padding='same', activation=activation)\n",
    "        x = tf.nn.dropout(x, keep_prob, name='6')\n",
    "        x = tf.contrib.layers.flatten(x)\n",
    "        \n",
    "        mu_Z = tf.layers.dense(x, name='7', units=n_latent)\n",
    "        mu_Z = tf.reshape(mu_Z, [n_X, 1, n_latent])\n",
    "        sigma_Z = tf.exp(0.5 * tf.layers.dense(x, name='8', units=n_latent) )\n",
    "        sigma_Z = tf.reshape(sigma_Z, [n_X, 1, n_latent])\n",
    "        return mu_Z, sigma_Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@tf.function\n",
    "def decoder(Z, keep_prob):\n",
    "    activation = lrelu\n",
    "    n_X = tf.shape(Z)[0]\n",
    "    n_MC = tf.shape(Z)[1]\n",
    "    with tf.name_scope(\"decoder\", reuse=tf.AUTO_REUSE):\n",
    "        x = tf.keras.layers.Dense(name='1', units=7*7*dec_in_channels//2, activation=activation)(Z)\n",
    "        x = tf.keras.layers.Dense(name='2' ,units=7*7*dec_in_channels, activation=activation)(x)\n",
    "        x = tf.reshape(x, [n_X*n_MC, 7, 7, dec_in_channels])\n",
    "        x = tf.keras.layers.Convolution2DTranspose(x, name='3', filters=64, kernel_size=4, strides=2, padding='same', activation=activation)(x)\n",
    "        x = tf.nn.dropout(x, keep_prob, name='4')(x)\n",
    "        x = tf.keras.layers.Convolution2DTranspose(x, name='5', filters=64, kernel_size=4, strides=1, padding='same', activation=activation)(x)\n",
    "        x = tf.nn.dropout(x, keep_prob, name='6')(x)\n",
    "        x = tf.keras.layers.Convolution2DTranspose(x, name='7', filters=64, kernel_size=4, strides=1, padding='same', activation=activation)(x)\n",
    "        x = tf.reshape(x, [n_X, n_MC, 14*14*64*dec_in_channels])\n",
    "        x = tf.keras.layers.Dense(x, name='8', units=28*28, activation=tf.nn.sigmoid)\n",
    "        img = tf.reshape(x, shape=[n_X, n_MC, 28, 28])\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(Z, keep_prob):\n",
    "    activation = lrelu\n",
    "    n_X = tf.shape(Z)[0]\n",
    "    n_MC = tf.shape(Z)[1]\n",
    "    with tf.variable_scope(\"decoder\", reuse=tf.AUTO_REUSE):\n",
    "        x = tf.layers.dense(Z, name='1', units=7*7*dec_in_channels//2, activation=activation)\n",
    "        x = tf.layers.dense(x, name='2' ,units=7*7*dec_in_channels, activation=lrelu)\n",
    "        x = tf.reshape(x, [n_X*n_MC, 7, 7, dec_in_channels])\n",
    "        x = tf.layers.conv2d_transpose(x, name='3', filters=64, kernel_size=4, strides=2, padding='same', activation=tf.nn.relu)\n",
    "        x = tf.nn.dropout(x, keep_prob, name='4')\n",
    "        x = tf.layers.conv2d_transpose(x, name='5', filters=64, kernel_size=4, strides=1, padding='same', activation=tf.nn.relu)\n",
    "        x = tf.nn.dropout(x, keep_prob, name='6')\n",
    "        x = tf.layers.conv2d_transpose(x, name='7', filters=64, kernel_size=4, strides=1, padding='same', activation=tf.nn.relu)\n",
    "        x = tf.reshape(x, [n_X, n_MC, 14*14*64*dec_in_channels])\n",
    "        x = tf.layers.dense(x, name='8', units=28*28, activation=tf.nn.sigmoid)\n",
    "        img = tf.reshape(x, shape=[n_X, n_MC, 28, 28])\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in train_dataset:\n",
    "    print(d.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder(d, 0.8, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO(X, Z, mu_Z, sigma_Z): \n",
    "    \n",
    "    n_latent = tf.shape(Z)[2]\n",
    "    \n",
    "    quad_pz_ary = tf.reduce_sum( -1./2. * Z**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_qz_ary = tf.reduce_sum( +1./(2. * sigma_Z**2) * (Z - mu_Z)**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_px_ary = tf.reduce_sum( -1./(2. * sigma_X**2) * (mu_X - X)**2, axis=[2,3]) #shape: [batch_size, n_MC]\n",
    "\n",
    "    #MC est. of E_q [logp(X|Z)] w.r.t. X, Z\n",
    "    img_loss = - (- (1/2.)* 28*28 *       tf.log(2 * math.pi * sigma_X**2)          + quad_px_ary \\\n",
    "                  + (1/2.)*tf.reduce_sum( tf.log(2 * math.pi * sigma_Z**2), axis=2) + quad_qz_ary)\n",
    "    img_loss = tf.reduce_mean(img_loss)#squeeze dim of n_MC\n",
    "\n",
    "    #E_q [P(Z) - logq(Z|X)] analytical value, \n",
    "    #it is a negative KL divergence (not between p(Z|X) and q(Z|X))\n",
    "    latent_loss = - (- (1/2.)*tf_float(n_latent)*np.log(2 * math.pi) + quad_pz_ary) \n",
    "    latent_loss = tf.reduce_mean(latent_loss) #squeeze dim of n_MC\n",
    "\n",
    "    return -(img_loss+latent_loss), img_loss, latent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO(mu_X, X, Z, mu_Z, sigma_Z): \n",
    "    \n",
    "    n_latent = tf.shape(Z)[2]\n",
    "    \n",
    "    quad_pz_ary = tf.reduce_sum( -1./2. * Z**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_qz_ary = tf.reduce_sum( +1./(2. * sigma_Z**2) * (Z - mu_Z)**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_px_ary = tf.reduce_sum( -1./(2. * sigma_X**2) * (mu_X - X)**2, axis=[2,3]) #shape: [batch_size, n_MC]\n",
    "\n",
    "    #MC est. of E_q [logp(X|Z)] w.r.t. X, Z\n",
    "    img_loss = - (- (1/2.)* 28*28 *       tf.log(2 * math.pi * sigma_X**2)          + quad_px_ary \\\n",
    "                  + (1/2.)*tf.reduce_sum( tf.log(2 * math.pi * sigma_Z**2), axis=2) + quad_qz_ary)\n",
    "    img_loss = tf.reduce_mean(img_loss)#squeeze dim of n_MC\n",
    "\n",
    "    #E_q [P(Z) - logq(Z|X)] analytical value, \n",
    "    #it is a negative KL divergence (not between p(Z|X) and q(Z|X))\n",
    "    latent_loss = - (- (1/2.)*tf_float(n_latent)*np.log(2 * math.pi) + quad_pz_ary) \n",
    "    latent_loss = tf.reduce_mean(latent_loss) #squeeze dim of n_MC\n",
    "\n",
    "    return -(img_loss+latent_loss), img_loss, latent_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IWAE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_IWELBO(mu_X, X, Z, mu_Z, sigma_Z):    \n",
    "    \n",
    "    #Importance MC est. of E_p[ \"exp. part of P(X|z)\" ] given X w.r.t. Z\n",
    "    #for numerical stability, a little complicated\n",
    "    \n",
    "    n_X = tf.shape(X)[0]\n",
    "    n_latent = tf.shape(mu_Z)[2]\n",
    "    \n",
    "    quad_pz_ary = tf.reduce_sum( 1./2. * Z**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_qz_ary = tf.reduce_sum( 1./(2. * sigma_Z**2) * (Z - mu_Z)**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_px_ary = tf.reduce_sum( 1./(2. * sigma_X**2) * (X - mu_X)**2, axis=[2,3]) #shape: [batch_size, n_MC]\n",
    "    \n",
    "    quad_ary = - quad_pz_ary + quad_qz_ary - quad_px_ary\n",
    "    \n",
    "    IWELBO = (1/2.) * tf.reduce_sum( tf.reshape(tf.log(sigma_Z**2), [n_X, n_latent]), axis=1)\\\n",
    "           - (1/2.)*28*28 * tf.log(2 * math.pi * sigma_X**2)\\\n",
    "           + tf_reduce_logmeanexp(quad_ary, axis=1)\n",
    "\n",
    "    return IWELBO #shape: [batch_size] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_gamma_div(mu_X, X, Z, mu_Z, sigma_Z, gamma):    \n",
    "    \n",
    "    #Importance MC est. of E_p[ \"exp. part of P(X|z)\" ] given X w.r.t. Z\n",
    "    #for numerical stability, a little complicated\n",
    "    \n",
    "    n_X = tf.shape(X)[0]\n",
    "    n_latent = tf.shape(Z)[2]\n",
    "\n",
    "    quad_pz_ary = tf.reduce_sum( 1./2. * Z**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_qz_ary = tf.reduce_sum( 1./(2. * sigma_Z**2) * (Z - mu_Z)**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_px_ary = tf.reduce_sum( 1./(2. * sigma_X**2) * (X - mu_X)**2, axis=[2,3]) #shape: [batch_size, n_MC]\n",
    "    \n",
    "    quad_ary = - quad_pz_ary + quad_qz_ary - quad_px_ary\n",
    "    \n",
    "    gamma_div = (1/2.) * tf.reduce_sum( tf.reshape(tf.log(sigma_Z**2), [n_X, n_latent]), axis=1)\\\n",
    "           - (1/2.)*28*28 * tf.log(2 * math.pi * sigma_X**2)\\\n",
    "           + (1/gamma) * tf_reduce_logmeanexp(gamma * quad_ary, axis=1)\n",
    "\n",
    "    return gamma_div #shape: [batch_size] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointwise_Pearson_UBO = lambda mu_X, X, Z, mu_Z, sigma_Z: pointwise_gamma_div(mu_X, X, Z, mu_Z, sigma_Z, gamma=2.)\n",
    "pointwise_Hellinger_LBO = lambda mu_X, X, Z, mu_Z, sigma_Z: pointwise_gamma_div(mu_X, X, Z, mu_Z, sigma_Z, gamma=0.5)\n",
    "pointwise_Neyman_LBO = lambda mu_X, X, Z, mu_Z, sigma_Z: pointwise_gamma_div(mu_X, X, Z, mu_Z, sigma_Z, gamma=-1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_reversed_gamma_div(mu_X, X, Z, mu_Z, sigma_Z, gamma):    \n",
    "    \n",
    "    #Importance MC est. of E_p[ \"exp. part of P(X|z)\" ] given X w.r.t. Z\n",
    "    #for numerical stability, a little complicated\n",
    "    \n",
    "    n_X = tf.shape(X)[0]\n",
    "    n_latent = tf.shape(Z)[2]\n",
    "\n",
    "    quad_pz_ary = tf.reduce_sum( 1./2. * Z**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_qz_ary = tf.reduce_sum( 1./(2. * sigma_Z**2) * (Z - mu_Z)**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_px_ary = tf.reduce_sum( 1./(2. * sigma_X**2) * (X - mu_X)**2, axis=[2,3]) #shape: [batch_size, n_MC]\n",
    "    \n",
    "    quad_ary = - quad_pz_ary + quad_qz_ary - quad_px_ary\n",
    "    \n",
    "    first_term = -((gamma-1)/2.) * tf.reduce_sum( tf.reshape(tf.log(sigma_Z**2), [n_X, n_latent]), axis=1)\\\n",
    "           + ((gamma-1)/2.)*28*28 * tf.log(2 * math.pi * sigma_X**2)\\\n",
    "           + tf_reduce_logmeanexp( - (gamma-1) * quad_ary, axis=1)\n",
    "    first_term *= - (1/gamma)\n",
    "    \n",
    "    second_term = (1/2.) * tf.reduce_sum( tf.reshape(tf.log(sigma_Z**2), [n_X, n_latent]), axis=1)\\\n",
    "           - (1/2.)*28*28 * tf.log(2 * math.pi * sigma_X**2)\\\n",
    "           + tf_reduce_logmeanexp(quad_ary, axis=1)\n",
    "    second_term *= (1/gamma)\n",
    "\n",
    "    return first_term + second_term #shape: [batch_size] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointwise_Pearson_LBO = lambda mu_X, X, Z, mu_Z, sigma_Z: pointwise_reversed_gamma_div(mu_X, X, Z, mu_Z, sigma_Z, gamma=2.)\n",
    "pointwise_Hellinger_UBO = lambda mu_X, X, Z, mu_Z, sigma_Z: pointwise_reversed_gamma_div(mu_X, X, Z, mu_Z, sigma_Z, gamma=0.5)\n",
    "pointwise_Neyman_UBO = lambda mu_X, X, Z, mu_Z, sigma_Z: pointwise_reversed_gamma_div(mu_X, X, Z, mu_Z, sigma_Z, gamma=-1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_reversed_KL_UBO(mu_X, X, Z, mu_Z, sigma_Z):    \n",
    "    \n",
    "    #Importance MC est. of E_p[ \"exp. part of P(X|z)\" ] given X w.r.t. Z\n",
    "    #for numerical stability, a little complicated\n",
    "    \n",
    "    n_X = tf.shape(X)[0]\n",
    "    n_latent = tf.shape(Z)[2]\n",
    "\n",
    "    quad_pz_ary = tf.reduce_sum( 1./2. * Z**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_qz_ary = tf.reduce_sum( 1./(2. * sigma_Z**2) * (Z - mu_Z)**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_px_ary = tf.reduce_sum( 1./(2. * sigma_X**2) * (X - mu_X)**2, axis=[2,3]) #shape: [batch_size, n_MC]\n",
    "    \n",
    "    quad_ary = - quad_pz_ary + quad_qz_ary - quad_px_ary\n",
    "    \n",
    "    log_w = (1/2.) * tf.reduce_sum( tf.reshape(tf.log(sigma_Z**2), [n_X, n_latent,1]), axis=1)\\\n",
    "           - (1/2.)*28*28 * tf.log(2 * math.pi * sigma_X**2)\\\n",
    "           + quad_ary\n",
    "    reversed_KL = tf.nn.softmax(log_w, axis=1)*log_w #shape: [batch_size, n_MC]\n",
    "    reversed_KL = tf.reduce_sum(reversed_KL, axis=1)\n",
    "    \n",
    "    return reversed_KL #shape: [batch_size] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_mutual_information(mu_X, X, Z, mu_Z, sigma_Z):    \n",
    "    \n",
    "    #Importance MC est. of E_p[ \"exp. part of P(X|z)\" ] given X w.r.t. Z\n",
    "    #for numerical stability, a little complicated\n",
    "    \n",
    "    n_X = tf.shape(X)[0]\n",
    "    n_latent = tf.shape(Z)[2]\n",
    "\n",
    "    quad_pz_ary = tf.reduce_sum( 1./2. * Z**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_qz_ary = tf.reduce_sum( 1./(2. * sigma_Z**2) * (Z - mu_Z)**2, axis=2) #shape: [batch_size, n_MC]\n",
    "    quad_px_ary = tf.reduce_sum( 1./(2. * sigma_X**2) * (X - mu_X)**2, axis=[2,3]) #shape: [batch_size, n_MC]\n",
    "    \n",
    "    quad_ary = - quad_pz_ary + quad_qz_ary - quad_px_ary\n",
    "    \n",
    "    log_w = (1/2.) * tf.reduce_sum( tf.reshape(tf.log(sigma_Z**2), [n_X, n_latent,1]), axis=1)\\\n",
    "             - (1/2.)*28*28 * tf.log(2 * math.pi * sigma_X**2)\\\n",
    "             + quad_ary\n",
    "    log_px = - (1/2.)*28*28 * tf.log(2 * math.pi * sigma_X**2)\\\n",
    "             - quad_px_ary\n",
    "    first_term = tf.nn.softmax(log_w, axis=1)*log_px #shape: [batch_size, n_MC]\n",
    "    first_term = tf.reduce_sum(first_term, axis=1) \n",
    "    second_term = - pointwise_IWELBO(mu_X, X, Z, mu_Z, sigma_Z)\n",
    "    \n",
    "    return first_term + second_term#shape: [batch_size] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLMC IWAE loss (non-adaptive, heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointwise_psi(l, X, pointwise_nested):\n",
    "\n",
    "    n_MC = 2**l\n",
    "    n_X = tf.shape(X)[0]\n",
    "    \n",
    "    mu_Z, sigma_Z = encoder(X, keep_prob)\n",
    "    Z = Z_sampler(mu_Z, sigma_Z, n_MC)\n",
    "    mu_X = decoder(Z, keep_prob)\n",
    "    \n",
    "    nested_whole = pointwise_nested(mu_X, X, Z, mu_Z, sigma_Z)\n",
    "\n",
    "    Z_1st_half = Z[:, :n_MC//2 , :]\n",
    "    Z_2nd_half = Z[:,  n_MC//2:, :]\n",
    "    \n",
    "    mu_X_1st_half = mu_X[:, :n_MC//2 , :, :]\n",
    "    mu_X_2nd_half = mu_X[:,  n_MC//2:, :, :]\n",
    "    \n",
    "    nested_1st_half = pointwise_nested(mu_X_1st_half, X, Z_1st_half, mu_Z, sigma_Z)\n",
    "    nested_2nd_half = pointwise_nested(mu_X_2nd_half, X, Z_2nd_half, mu_Z, sigma_Z)\n",
    "    \n",
    "    #psis is a [batch_size] dimentional tensor \n",
    "    pointwise_psi = nested_whole - (1./2.) * ( nested_1st_half + nested_2nd_half )\n",
    "    return pointwise_psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLMC_est(X, pointwise_nested, start_level, max_level, n_X, verbose=False): \n",
    "\n",
    "    levels = list(range(start_level, max_level+1))\n",
    "    Ns = {}\n",
    "    Cs = {}\n",
    "    NCs = {}\n",
    "    weight = np.array([2**(-3/2*l) for l in range(max_level+1)])\n",
    "    weight[:start_level] = 0\n",
    "    \n",
    "    for l in levels:\n",
    "        Ns[l] = math.ceil( (weight[l]/weight.sum()) * n_X ) \n",
    "        Cs[l] = 2**l\n",
    "        NCs[l] = Ns[l]*Cs[l]\n",
    "    \n",
    "    Ns[start_level] = n_X - sum([Ns[l] for l in levels[1:]])\n",
    "    if verbose:\n",
    "        print( {'N':Ns, 'C':Cs, 'NC':NCs, 'total_cost':sum(NCs.values())})\n",
    "    #MC est. of - E_p[log E_p[P(X|Z_sample)] |X ] w.r.t. X, psi_sample\n",
    "    \n",
    "    mu_Z, sigma_Z = encoder(X[:Ns[start_level]], keep_prob)\n",
    "    Z = Z_sampler(mu_Z, sigma_Z, Cs[start_level])\n",
    "    mu_X = decoder(Z, keep_prob)\n",
    "    \n",
    "    obj = tf.reduce_mean( pointwise_nested(mu_X, X[:Ns[start_level]], Z, mu_Z, sigma_Z) )\n",
    "    \n",
    "    for l in levels[1:]:\n",
    "        start_idx = sum(list(Ns.values())[:l-start_level])\n",
    "        obj += tf.reduce_mean( pointwise_psi(l, X[start_idx : start_idx+Ns[l]], pointwise_IWELBO) )\n",
    "        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode, sample, encode\n",
    "X = train_images.\n",
    "mu_Z, sigma_Z = encoder(X, keep_prob)\n",
    "Z = Z_sampler(mu_Z, sigma_Z, n_MC=1)\n",
    "mu_X = decoder(Z, keep_prob)\n",
    "\n",
    "_, latent_loss, img_loss = ELBO(mu_X, X, Z, mu_Z, sigma_Z)\n",
    "loss['ELBO'] = latent_loss + img_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode, sample, encode\n",
    "X = X0[:8]\n",
    "mu_Z, sigma_Z = encoder(X, keep_prob)\n",
    "Z = Z_sampler(mu_Z, sigma_Z, n_MC=32)\n",
    "mu_X = decoder(Z, keep_prob)\n",
    "\n",
    "IWELBO = pointwise_IWELBO(mu_X, X, Z, mu_Z, sigma_Z)\n",
    "loss['IWELBO'] = - tf.reduce_mean(IWELBO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss['MLMC-IWELBO'] = - MLMC_est(X0, pointwise_nested=pointwise_IWELBO, start_level=1, max_level=5, n_X=50, verbose=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconsrtruction based on importance sampling / Random Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X0[:1,:1,:,:]\n",
    "mu_Z,sigma_Z = encoder(X, keep_prob=1)\n",
    "reconstructed = decoder(mu_Z, keep_prob=1)\n",
    "\n",
    "Z = Z_sampler(tf.constant(np.zeros([1,1,n_latent],dtype=float_type)), 1, 1)\n",
    "generated = decoder(Z, keep_prob=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizing_ops(loss_p=None, loss_q=None, alpha_p=0.0005, alpha_q=0.0005):\n",
    "\n",
    "    optimizer_p = tf.train.AdamOptimizer(alpha_p, name='optimizer')\n",
    "    optimizer_q = tf.train.AdamOptimizer(alpha_q, name='optimizer')\n",
    "    \n",
    "    \n",
    "    ops = []\n",
    "    \n",
    "    if loss_p is not None:\n",
    "        ops.append(  )\n",
    "    if loss_q is not None:\n",
    "        ops.append( optimizer_q.minimize(loss_q, var_list=param_enc) )\n",
    "    return ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.0005, name='optimizer')\n",
    "\n",
    "param_dec = tf.trainable_variables(scope='decoder')\n",
    "param_enc = tf.trainable_variables(scope='encoder')\n",
    "\n",
    "op_elbo_p = optimizer.minimize(loss['ELBO'], var_list=param_dec)\n",
    "op_elbo_q = optimizer.minimize(loss['ELBO'], var_list=param_enc)\n",
    "\n",
    "ops = [op_elbo_q, op_elbo_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16,16])\n",
    "n_iter = 1000\n",
    "losses = []\n",
    "\n",
    "for i in range(n_iter):\n",
    "    batch = mnist.train.next_batch(batch_size=64)[0].reshape([-1,28,28])\n",
    "    _ = sess.run(ops, feed_dict = {X_in: batch, keep_prob: 0.8})\n",
    "        \n",
    "    if 0 == i % (n_iter//50):\n",
    "        j = i//(n_iter//50)\n",
    "        ls, r, i_ls, l_ls = sess.run([loss['ELBO'], reconstructed, img_loss, latent_loss], feed_dict = {X_in: batch, keep_prob: 1.0})\n",
    "        losses.append(ls)\n",
    "        plt.subplot(10,10,j//10*20+j%10+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(np.reshape(batch[0], [28, 28]), cmap='gray')\n",
    "        plt.title('#epoch:%0.0d'%(i))\n",
    "        plt.subplot(10,10,j//10*20+10+j%10+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(r[0,0,:,:], cmap='gray')\n",
    "        print(\"#epochs:%0.0d,\\tloss:%0.2f,\\timage_loss:%0.2f,\\tlatent_loss:%0.2f\"%(i, ls, np.mean(i_ls), np.mean(l_ls)))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLMC condition check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the variance of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(ops, n_step):\n",
    "    for i in range(n_step):\n",
    "        batch = mnist.train.next_batch(batch_size=batch_size)[0]\n",
    "        batch = batch.reshape([-1,28,28])\n",
    "        sess.run(ops, feed_dict = {X_in: batch, keep_prob: 0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dec_cost_mlmc(start_level, max_level, n_X):\n",
    "    levels = list(range(start_level, max_level+1))\n",
    "    Ns = {}\n",
    "    Cs = {}\n",
    "    NCs = {}\n",
    "    weight = np.array([2**(-3/2*l) for l in range(max_level+1)])\n",
    "    weight[:start_level] = 0\n",
    "\n",
    "    for l in levels:\n",
    "        Ns[l] = math.ceil( (weight[l]/weight.sum()) * n_X ) \n",
    "        Cs[l] = 2**l\n",
    "        NCs[l] = Ns[l]*Cs[l]\n",
    "    \n",
    "    return (sum(list(NCs.values())))\n",
    "\n",
    "def eval_mean_and_var(obj_list, n_sample):\n",
    "    mean_list = []\n",
    "    var_list = []\n",
    "    for obj in obj_list:\n",
    "        batches = [mnist.test.next_batch(batch_size=batch_size)[0].reshape([-1,28,28]) for i in range(n_sample)]\n",
    "        vals = [sess.run([obj], feed_dict = {X_in: batch, keep_prob: 1.0}) for batch in batches]\n",
    "        mean_list.append(np.mean(vals))\n",
    "        var_list.append(np.var(vals))\n",
    "    return mean_list, var_list\n",
    "\n",
    "def eval_standard_complexity(pointwise_nested, start_level, max_level, n_sample=20):\n",
    "    X = X0[:10]\n",
    "    psis = []\n",
    "    nested = []\n",
    "    mlmc = []\n",
    "    \n",
    "    dec_cost_nested = []\n",
    "    enc_cost_nested = []\n",
    "    dec_cost_mlmc = []\n",
    "    enc_cost_mlmc = []\n",
    "    \n",
    "    for l in range(start_level,max_level+1):\n",
    "        \n",
    "        mu_Z, sigma_Z = encoder(X, keep_prob)\n",
    "        Z = Z_sampler(mu_Z, sigma_Z, 2**l)\n",
    "        mu_X = decoder(Z, keep_prob)\n",
    "        \n",
    "        psis.append( tf.reduce_mean( pointwise_psi(l+1, X, pointwise_nested) ) )\n",
    "        nested.append( tf.reduce_mean( pointwise_nested(mu_X, X, Z, mu_Z, sigma_Z) ) ) \n",
    "        mlmc.append( MLMC_est(tf.reshape(X0[:100], [100, 1, 28, 28]), pointwise_nested=pointwise_IWELBO, start_level=start_level, max_level=l, n_X=100) )\n",
    "\n",
    "        dec_cost_nested.append(10)\n",
    "        enc_cost_nested.append(10*2**l)\n",
    "        dec_cost_mlmc.append(100)\n",
    "        enc_cost_mlmc.append( get_dec_cost_mlmc(start_level, l, n_X=100) )\n",
    "        \n",
    "    mean_psi, var_psi    = eval_mean_and_var(psis, n_sample)\n",
    "    _,        var_nested = eval_mean_and_var(nested, n_sample)\n",
    "    _,        var_mlmc   = eval_mean_and_var(mlmc, n_sample)\n",
    "    \n",
    "    cost_nested = np.array(enc_cost_nested) * (np.array(var_nested) / (4 * np.array(var_psi + np.array(mean_psi)**2)))\n",
    "    cost_mlmc   = np.array(enc_cost_mlmc  ) * (np.array(var_mlmc  ) / (4 * np.array(var_psi + np.array(mean_psi)**2)))\n",
    "\n",
    "    #print(var_nested)\n",
    "    #print(var_mlmc)\n",
    "    #print(var_psi)\n",
    "    \n",
    "    #print(enc_cost_nested)\n",
    "    #print(enc_cost_mlmc)\n",
    "    \n",
    "    return np.array([4 * np.sqrt(var_psi + np.array(mean_psi)**2), cost_nested, cost_mlmc]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_cost_accuracy(pointwise_nested, obj_name, start_level=0, max_level=7, n_sample=100):\n",
    "    r = eval_standard_complexity(pointwise_nested, start_level, max_level, n_sample)\n",
    "    x = r[:,0]/max(r[:,0])\n",
    "    x0 = x[0]\n",
    "    y = r[:,[1,2]]\n",
    "    y0 = y[0,:] = np.mean(y[0,:])\n",
    "    plt.plot(x,y)\n",
    "    plt.plot(x0*2.**(-np.arange(len(x))), y0*2.**(2*np.arange(len(y))), color='gray')\n",
    "    plt.plot(x0*2.**(-np.arange(len(x))), y0*2.**(3*np.arange(len(y))), color='gray')\n",
    "    plt.xlim([max(x)*1.1, min(x)*0.9])\n",
    "    plt.ylim([min(y[:,0])*0.5, max(y[:,0])*2])\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('relative accuracy: epsilon')\n",
    "    plt.ylabel('required cost')\n",
    "    plt.legend(['cost of nested estimator', 'cost of MLMC estimator', 'O(1/epsilon^2)', 'O(1/epsilon^3)'])\n",
    "    plt.savefig('../out/plot/cost_accuracy_{}.eps'.format(obj_name))\n",
    "    save(r, '../out/log/cost_accuracy_{}.pickle'.format(obj_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check of the order of psi (difference of estimators between levels) and IWELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergenvce(pointwise_nested, start_level, max_level, n_sample=20, figname=None):\n",
    "    \n",
    "    levels = np.arange(start_level,max_level+1)\n",
    "    psis = []\n",
    "    \n",
    "    for l in levels:\n",
    "        psis.append( tf.reduce_mean( pointwise_psi(l, X0[:10], pointwise_nested) ) )\n",
    "    \n",
    "    mean_psi, var_psi = eval_mean_and_var(psis, n_sample)\n",
    "    var_psi = np.array(var_psi)*10\n",
    "    mean_abs_psi = np.abs(mean_psi)\n",
    "    \n",
    "    plt.figure(figsize=[8,6])\n",
    "    plt.plot(levels,var_psi)\n",
    "    plt.plot(levels,mean_abs_psi)\n",
    "    plt.plot(levels,[var_psi[0]*2.**(-2*l) for l in range(len(levels))],c='gray')\n",
    "    plt.plot(levels,[mean_abs_psi[0]*2.**(-l) for l in range(len(levels))],c='gray')\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('variance')\n",
    "    plt.xlabel('level')\n",
    "    plt.legend([\n",
    "        'Var_X[ psi_l(X) ]\\n',\n",
    "        '|E_X[ psi_l(X) ]|\\n',\n",
    "        'O(2^(-2l))', 'O(2^(-l))'\n",
    "    ])\n",
    "    if figname is not None:\n",
    "        plt.savefig('../out/plot/psi_convergence_{}.eps'.format(figname))\n",
    "        save(np.array([var_psi, mean_abs_psi]), '../out/log/psi_convergence_{}.pickle'.format(figname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the variance of gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mean_and_var_grad(obj, n_sample):\n",
    "    \n",
    "    mean_list = []\n",
    "    var_list = []\n",
    "    \n",
    "    g = tf.train.GradientDescentOptimizer(0.0005)\n",
    "    grads = g.compute_gradients(obj)\n",
    "    grads_flat = tf.concat([tf.reshape(g_tuple[0],[-1]) for g_tuple in grads], axis=0)\n",
    "    \n",
    "    mean = 0\n",
    "    mean_sq = 0\n",
    "    for i in range(n_sample):\n",
    "        batch = mnist.test.next_batch(batch_size=batch_size)[0].reshape([-1,28,28])\n",
    "        tmp_grads = np.float64(sess.run(grads_flat, feed_dict = {X_in: batch, keep_prob: 1.0}))\n",
    "        mean    += (1/n_sample) * np.abs(tmp_grads)\n",
    "        mean_sq += (1/n_sample) * tmp_grads**2\n",
    "    var = mean_sq - mean**2\n",
    "    \n",
    "    return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergenvce_grad(pointwise_nested, start_level, max_level, n_sample=20, figname=None):\n",
    "    \n",
    "    levels = np.arange(start_level,max_level+1)\n",
    "    moment2 = []\n",
    "    moment1 = []\n",
    "    \n",
    "    for l in levels:\n",
    "        psi = tf.reduce_mean( pointwise_psi(l, X0[:10], pointwise_nested) )\n",
    "        mean_psi_grad, var_psi_grad = eval_mean_and_var_grad(psi, n_sample)\n",
    "        moment2.append( sum(var_psi_grad)*10 )\n",
    "        moment1.append( sum(np.abs(mean_psi_grad)) )\n",
    "\n",
    "    plt.figure(figsize=[8,6])\n",
    "    plt.plot(levels, moment2)\n",
    "    plt.plot(levels, moment1)\n",
    "    plt.plot(levels,[moment2[0]*2.**(-2*l) for l in range(len(levels))],c='gray')\n",
    "    plt.plot(levels,[moment1[0]*2.**(-l) for l in range(len(levels))],c='gray')\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('variance')\n",
    "    plt.xlabel('level')\n",
    "    plt.legend([\n",
    "        '||Var_X[ nabla psi_l(X) ]||_tr',\n",
    "        '||E_X[ nabla psi_l(X) ]||_1',\n",
    "        'O(2^(-2l))', 'O(2^(-l))'\n",
    "    ])\n",
    "    if figname is not None:\n",
    "        plt.savefig('../out/plot/psi_grad_convergence_{}.eps'.format(figname))\n",
    "        save(np.array([moment1, moment2]), '../out/log/psi_grad_convergence_{}.pickle'.format(figname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'iwelbo':pointwise_IWELBO,\n",
    "    'reversed_KL':pointwise_reversed_KL_UBO,\n",
    "    'mutual_info':pointwise_mutual_information,\n",
    "    'hellinger_lbo':pointwise_Hellinger_LBO,\n",
    "    'hellinger_ubo':pointwise_Hellinger_UBO,\n",
    "    'neyman_lbo':pointwise_Neyman_LBO,\n",
    "    'neyman_ubo':pointwise_Neyman_UBO,\n",
    "    'pearson_lbo':pointwise_Pearson_LBO,\n",
    "    'pearson_ubo':pointwise_Pearson_UBO\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "train_model(ops, n_step=200)\n",
    "\n",
    "for obj_name, pointwise_nested in metrics.items():\n",
    "    plot_cost_accuracy(pointwise_nested, '{}_{}_step_temp'.format(obj_name, 200), start_level=0, max_level=7, n_sample=100)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "#train_model(ops, n_step=200)\n",
    "\n",
    "for obj_name, pointwise_nested in metrics.items():\n",
    "    #plot_cost_accuracy(pointwise_nested, '{}_{}_step'.format(obj_name, 200), start_level=0, max_level=7, n_sample=100)\n",
    "    print('completed: {}_{}_step'.format(obj_name, 200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [200, 300, 400, 500, 1000, 1500]\n",
    "delta_steps = np.array(steps[1:]) - np.array(steps[:-1])\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "if steps[0]!=0:\n",
    "    train_model(ops, n_step=steps[0])\n",
    "    \n",
    "for obj_name, pointwise_nested in metrics.items():\n",
    "    plot_convergenvce(pointwise_nested, start_level=1, max_level=7, n_sample=100, \n",
    "                      figname='{}_{}_step'.format(obj_name, steps[0]))\n",
    "    plot_convergenvce_grad(pointwise_nested, start_level=1, max_level=7, n_sample=100, \n",
    "                      figname='{}_{}_step'.format(obj_name, steps[0]))\n",
    "    print('completed: {}_{}_step'.format(obj_name, steps[0]))\n",
    "\n",
    "for i in range(len(delta_steps)):\n",
    "    train_model(ops, n_step=delta_steps[i])\n",
    "    for obj_name, pointwise_nested in metrics.items():\n",
    "        plot_convergenvce(pointwise_nested, start_level=1, max_level=7, n_sample=100, \n",
    "                          figname='{}_{}_step'.format(obj_name, steps[i+1]))\n",
    "        plot_convergenvce_grad(pointwise_nested, start_level=1, max_level=7, n_sample=100, \n",
    "                          figname='{}_{}_step'.format(obj_name, steps[i+1]))\n",
    "        print('completed: {}_{}_step'.format(obj_name, steps[i+1]))\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of IWELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_psi_convergence_plots(pointwise_nested, steps, obj_name, train_ops):\n",
    "    delta_steps = np.array(steps[1:]) - np.array(steps[:-1])\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    if steps[0]!=0:\n",
    "        train_model(train_ops, n_step=steps[0])\n",
    "    plot_convergenvce(pointwise_nested, start_level=1, max_level=9, n_sample=100, \n",
    "                      figname='../out/{}_psi_convergence_{}_step.eps'.format(obj_name,steps[0]))\n",
    "    \n",
    "    for i in range(len(delta_steps)):\n",
    "        train_model(train_ops, n_step=delta_steps[i])\n",
    "        plot_convergenvce(pointwise_nested, start_level=1, max_level=9, n_sample=100, \n",
    "                      figname='../out/{}_psi_convergence_{}_step.eps'.format(obj_name, steps[i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_psi_grad_convergence_plots(pointwise_nested, steps, obj_name, train_ops):\n",
    "    delta_steps = np.array(steps[1:]) - np.array(steps[:-1])\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    if steps[0]!=0:\n",
    "        train_model(train_ops, n_step=steps[0])\n",
    "    plot_convergenvce_grad(pointwise_nested, start_level=1, max_level=9, n_sample=100, \n",
    "                      figname='../out/psi_grad_convergence_{}_{}_step.eps'.format(obj_name,steps[0]))\n",
    "    \n",
    "    for i in range(len(delta_steps)):\n",
    "        train_model(train_ops, n_step=delta_steps[i])\n",
    "        plot_convergenvce_grad(pointwise_nested, start_level=1, max_level=9, n_sample=100, \n",
    "                      figname='../out/{}_psi_grad_convergence_{}_step.eps'.format(obj_name,steps[i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_psi_convergence_plots(pointwise_IWELBO, steps=[0,100,200,300,400,500,1000,1500], obj_name='iwelbo', train_ops=ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_psi_grad_convergence_plots(pointwise_IWELBO, steps=[0,100,200,300,400,500,1000,1500], obj_name='iwelbo', train_ops=ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cost_accuracy(pointwise_IWELBO, 'iwelbo', start_level=0, max_level=7, n_sample=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "get_psi_convergence_plots(pointwise_IWELBO, steps=[0,100,200,300,400,500,1000,1500], obj_name='iwelbo', train_ops=ops)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
