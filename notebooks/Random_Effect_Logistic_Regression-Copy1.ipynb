{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Effect Logistic Regression by MLMC Variational Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Effect Models:\n",
    "For $n=1,2,...$,\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$Z_n \\sim N(0,\\tau^2)$\n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$Y_{n,t} \\sim \\text{Bernoulli}\\left(\\frac{1}{1+\\exp(- Z_n - \\beta_0 - \\beta^T x_{n,t})}\\right)$\n",
    "<br>\n",
    "for $t=1, ..., T$. This model carries out dimentionality reduction of binary observations $y_{n,k}$'s. Here, the dimention of $\\beta$ and $x_{n,t}$ is $D$.<br>\n",
    "As variational approximation of the posterior $p(z_n|y_n)$, we use $q(z_n)= N(z_n;\\mu_n, \\sigma_n^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To constrain the parameter $\\tau^2>0$, we parametrize $\\tau^2$ as $\\tau^2 = \\mathrm{softplus}(\\alpha)$, where $\\mathrm{softplus}(x) := \\log(1+\\exp(x))>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Possible Extension:\n",
    "\n",
    "By adding $\\bar x_n=\\frac{1}{T}\\sum_t x_{n,t}$ to the predictors as \n",
    "<br>&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$Y_{n,t} \\sim \\text{Bernoulli}\\left(\\frac{1}{1+\\exp(- Z_n - \\beta_0 - \\beta^T x_{n,t}- \\gamma\\bar x_n)}\\right)$,\n",
    "<br>\n",
    "we can obtain correlated random effect models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We do not consider the use of Renyi divergences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn GPUs off\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import bernoulli, norm\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.models.random_effect_logistic_regression import random_effect_logistic_regression as RELR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda x: 1/(1+np.exp(-x))\n",
    "softplus = lambda x: np.log(1+np.exp(x))\n",
    "\n",
    "def timestamp():\n",
    "    now = datetime.datetime.now()\n",
    "    return now.strftime(\"%Y%m%d%H%M%S\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "D = 3\n",
    "T = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramters\n",
    "alpha = np.float64(1.)\n",
    "beta0 = np.float64(0.)\n",
    "beta  = np.array([0.25, 0.50, 0.75]) #np.random.randn(D) / np.sqrt(D)\n",
    "param0 = {\n",
    "    'alpha': alpha,\n",
    "    'beta0': beta0,\n",
    "    'beta': beta\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume that we have infinite amount of data.\n",
    "# Thus, generator of the data is implemented.\n",
    "def generate_data(N, D, T, beta0, beta, alpha):\n",
    "    \"\"\"\n",
    "    Genarate N samples of data from the model with parameter [beta0, beta, alpha]. \n",
    "    Returns:\n",
    "    x: 3-d array of size [N, T, D]\n",
    "    y: 2-d array of size [N, T]\n",
    "    z: 1-d array of size [n_MC, N]\n",
    "    \"\"\"\n",
    "\n",
    "    z = np.random.randn(N) * softplus(alpha)**(1/2.)\n",
    "    x = np.random.randn(N*T*D).reshape([N,T,D])\n",
    "    y = bernoulli(p=sigmoid(beta0+x@beta+z.reshape([N,1]))).rvs()\n",
    "    return x,y,z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To constrain the parameter $\\tau^2>0$, we parametrize $\\tau^2$ as $\\tau^2 = \\mathrm{softplus}(\\alpha)$, where $\\mathrm{softplus}(x) := \\log(1+\\exp(x))>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Normal Integral Approximation of Evidence\n",
    "\n",
    "Ref: Barber Bishop(1998), PRML(2006)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid normal integral approximation is:\n",
    "$$\\int \\sigma(z + \\mu)N(z;0,\\tau^2)\\mathrm{d}z \n",
    "\\approx \\sigma(\\kappa \\mu) \\ \\ \\ \n",
    "\\text{where}\\ \\ \\ \n",
    "\\kappa = \\left(\\frac{1}{1 + \\pi\\tau^2/8}\\right)^{\\frac{1}{2}}$$\n",
    "This is applied to the probability of y: \n",
    "$$Pr(Y=1|X=x) = \\int \\sigma(z + \\beta_0 + \\beta^T x)N(z;0,\\tau^2)\\mathrm{d}z,$$\n",
    "where $\\tau^2 = \\mathrm{softmax}(\\alpha)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Approximation of Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplace approximation of $p(z|x,y)$ is:\n",
    "$$p(z|x,y) \\approx \\exp\\left[\\frac{1}{2}\\left(\\frac{\\mathrm{d}^2}{\\mathrm{d}z^2}\\log p(\\hat z|x,y) \\right)(z - \\hat z)^2 + \\text{const.}\\right] = N\\left(z;\\hat z, \\left(-\\frac{\\mathrm{d}^2}{\\mathrm{d}z^2}\\log p(\\hat z|x,y) \\right)^{-1}\\right),$$\n",
    "where $\\hat z$ is the maximum a postriori (MAP), so that $\\hat z = \\arg\\max_{z}\\log p(z|x,y) = \\arg\\max_{z}\\log p(x,y|z)p(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IW-ELBO approximation of Evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance weighted evidence lower bound (IW-ELBO) is an approximation of the evidence, and its Monte Carlo estimator is defined as:\n",
    "$$\\mathrm{IW}\\text{-}\\mathrm{ELBO}(x,z_{1:K}):=\\frac{1}{N}\\sum_{n=1}^{N}\\log \\frac{1}{K}\\sum_{k=1}^{K}\\left(\\frac{p(x_n,z_{nk})}{q(z_{nk};x_n)}\\right),$$\n",
    "where $z_{nk}$ is sampled as $z_{nk}\\sim q(z;x_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance Analysis of SUMO:\n",
    "We assume that $\\mathbb{P}(k\\leq\\mathcal{K})=\\frac{1}{k}$ for $\\mathcal{K}\\leq K_{\\text{max}}$, and accordingly, $\\mathbb{P}(k=\\mathcal{K}<K_{\\text{max}})=\\frac{1}{k(k+1)}$. Also, we know convergence rate of $\\Delta_k:=\\mathrm{IWELBO(x,z_{1:k})} - \\mathrm{IWELBO(x,z_{1:k-1})}$ is as following:\n",
    "\n",
    "- $\\mathrm{E}|\\Delta_k| = \\mathcal{O}(1/k)$\n",
    "- $\\mathrm{E}|\\Delta_k|^2 = \\mathcal{O}(1/k^2) \\geq \\mathrm{Var}[\\Delta_k]$\n",
    "- $\\mathrm{E}[\\Delta_k] = \\mathcal{O}(1/k^2)$　　　←（∵$\\sum_{k=K+1}^{\\infty}\\mathrm{E}[\\Delta_k] = \\mathcal{O}(\\frac{1}{K})$ and $0\\leq\\mathrm{E}[\\Delta_k]$ for all $k$.）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using above properties, we can analyze the order of magnitude of the variance of SUMO as:\n",
    "\\begin{align}\n",
    "\\mathrm{Var}[SUMO_{\\leq K_{\\text{max}}}]\n",
    "&= \\mathrm{Var}_{\\mathcal{K}, \\Delta_{1:K_{\\text{max}}}} \n",
    "\\left( \\sum_{k=1}^{K_{\\text{max}}} \\frac { \\mathbb{1}_{(k\\leq\\mathcal{K})} } \n",
    "{ \\mathbb{P}(k\\leq\\mathcal{K}) } \\Delta_k \\right) \\\\\n",
    "&= \\mathrm{E}_{\\mathcal{K}}\\mathrm{Var}_{\\Delta_{1:K_{\\text{max}}}}\n",
    "\\left( \\sum_{k=1}^{\\mathcal{K}} k \\Delta_k | \\mathcal {K}\\right)\n",
    " + \\mathrm{Var}_{\\mathcal{K}}\\mathrm{E}_{\\Delta_{1:K_{\\text{max}}}}\n",
    "\\left( \\sum_{k=1}^{\\mathcal{K}} k \\Delta_k | \\mathcal {K}\\right) \\\\\n",
    "&= \\mathrm{E}_{\\mathcal{K}}\\sum_{k=1}^{\\mathcal{K}} k^2 \\mathrm{Var}_{\\Delta_{1:K_{\\text{max}}}}\n",
    "\\left( \\Delta_k | \\mathcal {K}\\right)\n",
    " + \\mathrm{Var}_{\\mathcal{K}} \\sum_{k=1}^{\\mathcal{K}} k \\mathrm{E}_{\\Delta_{1:K_{\\text{max}}}}\n",
    "\\left( \\Delta_k | \\mathcal {K}\\right) \\\\\n",
    "&= \\mathrm{E}_{\\mathcal{K}}\\sum_{k=1}^{\\mathcal{K}} k^2 \\mathcal{O}(1/k^2)\n",
    " + \\mathrm{Var}_{\\mathcal{K}} \\sum_{k=1}^{\\mathcal{K}} k \\mathcal{O}(1/k^2) \\\\\n",
    "&= \\mathrm{E}_{\\mathcal{K}} \\mathcal{O}(\\mathcal{K})\n",
    " + \\mathrm{Var}_{\\mathcal{K}}\\mathcal{O}(\\log \\mathcal{K}) \\\\\n",
    "&= \\mathcal{O}(\\log K_{\\text{max}}) + \\text{const.}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the expected complexity of SUMO is $\\mathrm{E}[\\mathcal{\\mathcal{K}}] = \\mathcal{O}(\\log K_{\\text{max}})$. Therefore, the \"variance per reciprocal complexity\" for SUMO is $\\mathcal{O}\\left((\\log K_{\\text{max}})^2\\right)$. Those for NMC and MLMC are $\\mathcal{O}(K_{\\text{max}})$ and $\\mathcal{O}(1)$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure that implementations are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x,y,_ = generate_data(N=10000, D=3, T=2, beta0=beta0, beta=beta, alpha=alpha)\n",
    "model = RELR(alpha, beta0, beta)\n",
    "q_param = model.laplace_approx(x, y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mu': array([ 0.85936059,  0.09936386, -0.00549784, ...,  0.67903061,\n",
       "         0.65668019,  0.84328928]),\n",
       " 'sigma': array([0.93545707, 0.9365725 , 0.89478043, ..., 0.94620007, 0.95501274,\n",
       "        0.91546504])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=-1.269810829237006>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sigmoid_normal_likelihood(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sigmoid_normal_likelihood' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3e3f878826bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msignorm_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid_normal_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0melbo_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIWELBO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_MC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0miwelbo_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIWELBO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_MC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0miwelbo_likelihood_mlmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIWELBO_MLMC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0miwelbo_likelihood_randmlmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIWELBO_MLMC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sigmoid_normal_likelihood' is not defined"
     ]
    }
   ],
   "source": [
    "signorm_likelihood = sigmoid_normal_likelihood(x, y, beta0, beta, alpha).numpy()\n",
    "elbo_likelihood = IWELBO(x, y, beta0, beta, alpha, mu, sigma, n_MC=1).numpy()\n",
    "iwelbo_likelihood = IWELBO(x, y, beta0, beta, alpha, mu, sigma, n_MC=64).numpy()\n",
    "iwelbo_likelihood_mlmc = IWELBO_MLMC(x, y, beta0, beta, alpha, mu, sigma, max_level=6, w0=0.9, randomize=False).numpy()\n",
    "iwelbo_likelihood_randmlmc = IWELBO_MLMC(x, y, beta0, beta, alpha, mu, sigma, max_level=6, w0=0.9, randomize=True).numpy()\n",
    "iwelbo_likelihood_sumo = IWELBO_SUMO(x, y, beta0, beta, alpha, mu, sigma, K_max=10000).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signorm_likelihood, elbo_likelihood, iwelbo_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iwelbo_likelihood_mlmc, iwelbo_likelihood_randmlmc, iwelbo_likelihood_sumo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLMC codition check for objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_stats_dIWELBO(x, y, beta0, beta, alpha, mu, sigma, level=1):\n",
    "    # Compute dIWELBO (and IWELBO) for each sample and \n",
    "    # summarize them into several statistics.\n",
    "\n",
    "    N, = mu.shape\n",
    "    n_MC = 2**level\n",
    "    z = norm(loc=mu, scale=sigma).rvs([n_MC, N])\n",
    "    \n",
    "    diwelbos = pointwise_dIWELBO(x, y, z, beta0, beta, alpha, mu, sigma).numpy()\n",
    "    iwelbos = pointwise_IWELBO(x, y, z, beta0, beta, alpha, mu, sigma).numpy()\n",
    "    \n",
    "    return {'mean_dIWELBO':np.mean(diwelbos), \n",
    "            'mean_abs_dIWELBO':np.mean(np.abs(diwelbos)), \n",
    "            'mean_squared_dIWELBO':np.mean(diwelbos**2),\n",
    "            'var_dIWELBO':np.var(diwelbos), \n",
    "            'var_IWELBO':np.var(iwelbos)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmp(l):\n",
    "    N0 = 2000000\n",
    "    x,y,_ = generate_data(N=N0//2**l, D=3, T=2, beta0=beta0, beta=beta, alpha=alpha)\n",
    "    mu, sigma = laplace_approx(x, y, beta0, beta, alpha)\n",
    "    return conv_stats_dIWELBO(x, y, beta0, beta, alpha, mu, sigma, level=l)\n",
    "L=13\n",
    "conv_stats = [tmp(l) for l in range(L)]\n",
    "conv_stats = pd.DataFrame(conv_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "plt.plot(conv_stats[['mean_abs_dIWELBO', 'var_dIWELBO', 'var_IWELBO']])\n",
    "\n",
    "# plot O(2^{-l/2}), O(2^{-l}), O(2^{-2l})\n",
    "s,t = conv_stats[['mean_abs_dIWELBO', 'var_dIWELBO']].iloc[0]\n",
    "plt.plot(t*2.**(-np.arange(L)/2), c='grey')\n",
    "plt.plot(t*2.**(-np.arange(L)), c='grey')\n",
    "plt.plot(t*2.**(-np.arange(L)*2), c='grey')\n",
    "\n",
    "plt.legend([r'$\\mathrm{E} | \\Delta \\mathrm{IW}$-$\\mathrm{ELBO}|$', \n",
    "            r'$\\mathrm{Var}[\\Delta \\mathrm{IW}$-$\\mathrm{ELBO}]$', \n",
    "            r'$\\mathrm{Var}[\\mathrm{IW}$-$\\mathrm{ELBO}]$',\n",
    "            r'$O(2^{-\\ell/2}), O(2^{-\\ell}), O(2^{-2\\ell})$'])\n",
    "plt.xlabel('Level')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# compute cost-variance efficiency of MLMC \n",
    "C = [1 if l==0 else 2**(l+1) for l in range(L)]\n",
    "V = conv_stats['var_dIWELBO']\n",
    "N = np.sqrt(V/C)\n",
    "cost_mlmc = [np.sum([np.sqrt(c*v) for c,v in zip(C[:l+1], V[:l+1])])**2  for l in range(L)]\n",
    "cost_nmc = [2**l * v for l,v in enumerate(conv_stats['var_IWELBO'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,4])\n",
    "# plot theoretical computational efficiency\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(cost_mlmc)\n",
    "plt.plot(cost_nmc)\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Variance per Reciprocal Complexity')\n",
    "plt.xlabel('level')\n",
    "plt.legend(['MLMC', 'Nested MC'])\n",
    "# plot N_l's for MLMC\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(N)\n",
    "plt.yscale('log')\n",
    "plt.ylabel(r'$N_l$')\n",
    "plt.xlabel('level')\n",
    "print('(Theoretical) Variance per Reciprocal Complexity and N_l\\'s for MLMC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "conv_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLMC codition check for gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def conv_stats_grad_dIWELBO(x, y, beta0, beta, alpha, mu, sigma, level=1):\n",
    "    # Compute the gradient of dIWELBO (and IWELBO) for each sample and \n",
    "    # summarize them into several statistics.\n",
    "\n",
    "    N, = mu.shape\n",
    "    n_MC = 2**level\n",
    "    z = norm(loc=mu, scale=sigma).rvs([n_MC, N]).T\n",
    "    \n",
    "    param = tf.concat([beta, [beta0], [alpha]], axis=0)\n",
    "    param = tf.Variable(param, dtype=tf.float64)\n",
    "    params = tf.reshape(param, [1,D+2]) * np.ones([N,1])\n",
    "\n",
    "    mu, sigma = laplace_approx(x, y, beta0, beta, alpha)        \n",
    "    \n",
    "    # Define a gradient function to be vectorized (vectorization for better performance)\n",
    "    def get_grad(args):\n",
    "        # get gradient of dIWELBO (and IWELBO) given one sample\n",
    "        param, x_, y_, z_, mu, sigma = args\n",
    "        z_ = tf.reshape(z_, [-1,1])\n",
    "        with tf.GradientTape(persistent=True) as g:\n",
    "            g.watch(param)\n",
    "            beta_ = param[0,:D]\n",
    "            beta0_ = param[0,D]\n",
    "            alpha_ = param[0,D+1]\n",
    "            diwelbos = pointwise_dIWELBO(x_, y_, z_, beta0_, beta_, alpha_, mu, sigma)\n",
    "            iwelbos = pointwise_IWELBO(x_, y_, z_, beta0_, beta_, alpha_, mu, sigma)    \n",
    "        a = g.gradient(diwelbos, param)\n",
    "        b = g.gradient(iwelbos, param)\n",
    "        del g\n",
    "        return a,b\n",
    "    \n",
    "    # Compute the gradient of dIWELBO (and IWELBO) for each sample\n",
    "    args = [tf.expand_dims(arg, axis=1) for arg in [params, x, y, z, mu, sigma]]\n",
    "    grads = tf.vectorized_map(get_grad, args)\n",
    "    \n",
    "    grad_diwelbos = tf.squeeze(grads[0])#[:D+1]\n",
    "    grad_iwelbos = tf.squeeze(grads[1])#[:D+1]\n",
    "    \n",
    "    # return summary statistics of the gradients\n",
    "    return {'norm_mean_grad_dIWELBO': np.linalg.norm(np.mean(grad_diwelbos, axis=0)), \n",
    "            'mean_norm_grad_dIWELBO': np.mean(np.linalg.norm(grad_diwelbos, axis=1)), \n",
    "            'mean_squared_norm_grad_dIWELBO': np.mean(np.linalg.norm(grad_diwelbos, axis=1)**2),\n",
    "            'trace_covariance_grad_dIWELBO': np.sum(np.var(grad_diwelbos, axis=0)), \n",
    "            'trace_covariance_grad_IWELBO': np.sum(np.var(grad_iwelbos, axis=0))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def tmp(l):\n",
    "    N0 = 2000000\n",
    "    x,y,_ = generate_data(N=N0//2**l, D=3, T=2, beta0=beta0, beta=beta, alpha=alpha)\n",
    "    mu, sigma = laplace_approx(x, y, beta0, beta, alpha)\n",
    "    return conv_stats_grad_dIWELBO(x, y, beta0, beta, alpha, mu, sigma, level=l)\n",
    "L=13\n",
    "conv_stats = [tmp(l) for l in range(L)]\n",
    "conv_stats = pd.DataFrame(conv_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "plt.plot(conv_stats[['norm_mean_grad_dIWELBO', 'trace_covariance_grad_dIWELBO', 'trace_covariance_grad_IWELBO']])\n",
    "\n",
    "# plot O(2^{-l/2}), O(2^{-l}), O(2^{-2l})\n",
    "s,t = conv_stats[['norm_mean_grad_dIWELBO', 'trace_covariance_grad_dIWELBO']].iloc[0]\n",
    "plt.plot(t*2.**(-np.arange(L)/2), c='grey')\n",
    "plt.plot(t*2.**(-np.arange(L)), c='grey')\n",
    "plt.plot(t*2.**(-np.arange(L)*2), c='grey')\n",
    "\n",
    "plt.legend([r'$||\\mathrm{E} [\\nabla (\\Delta \\mathrm{IW}$-$\\mathrm{ELBO})]||_2$', \n",
    "            r'$\\mathrm{tr}(\\mathrm{Cov}[\\nabla(\\Delta \\mathrm{IW}$-$\\mathrm{ELBO})])$', \n",
    "            r'$\\mathrm{tr}(\\mathrm{Cov}[\\nabla(\\mathrm{IW}$-$\\mathrm{ELBO})])$',\n",
    "            r'$O(2^{-\\ell/2}), O(2^{-\\ell}), O(2^{-2\\ell})$'])\n",
    "plt.xlabel('Level')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# compute cost-variance efficiency of MLMC \n",
    "C = [1 if l==0 else 2**(l+1) for l in range(L)]\n",
    "V = conv_stats['trace_covariance_grad_dIWELBO']\n",
    "N = np.sqrt(V/C)\n",
    "cost_mlmc = [np.sum([np.sqrt(c*v) for c,v in zip(C[:l+1], V[:l+1])])**2  for l in range(L)]\n",
    "cost_nmc = [2**l * v for l,v in enumerate(conv_stats['trace_covariance_grad_IWELBO'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,4])\n",
    "# plot theoretical computational efficiency\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(cost_mlmc)\n",
    "plt.plot(cost_nmc)\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Variance per Reciprocal Complexity')\n",
    "plt.xlabel('level')\n",
    "plt.legend(['MLMC', 'Nested MC'])\n",
    "# plot N_l's for MLMC\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(N)\n",
    "plt.yscale('log')\n",
    "plt.ylabel(r'$N_l$')\n",
    "plt.xlabel('level')\n",
    "print('(Theoretical) Variance per Reciprocal Complexity and N_l\\'s for MLMC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "conv_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def conv_stats_dSUMO(x, y, beta0, beta, alpha, mu, sigma, level=1):\n",
    "    # Compute dIWELBO (and IWELBO) for each sample and \n",
    "    # summarize them into several statistics.\n",
    "\n",
    "    N, = mu.shape\n",
    "    n_MC = 2**level\n",
    "    z = norm(loc=mu, scale=sigma).rvs([n_MC, N])\n",
    "    \n",
    "    dsumos = 0\n",
    "    dsumos += pointwise_IWELBO(x, y, z[ :,:], beta0, beta, alpha, mu, sigma).numpy()\n",
    "    dsumos -= pointwise_IWELBO(x, y, z[1:,:], beta0, beta, alpha, mu, sigma).numpy()\n",
    "    \n",
    "    dN = N//10\n",
    "    var_sumos = dN * np.var([\n",
    "        IWELBO_SUMO(\n",
    "            x[dN*i:dN*(i+1)], y[dN*i:dN*(i+1)], beta0, beta, alpha,\n",
    "            mu[dN*i:dN*(i+1)], sigma[dN*i:dN*(i+1)], K_max=2**level\n",
    "            ) \n",
    "        for i in range(10)])\n",
    "    \n",
    "    return {'mean_dSUMO':np.mean(dsumos), \n",
    "            'mean_abs_dSUMO':np.mean(np.abs(dsumos)), \n",
    "            'mean_squared_dSUMO':np.mean(dsumos**2),\n",
    "            'var_dSUMO':np.var(dsumos),\n",
    "            'var_SUMO':var_sumos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def tmp(l):\n",
    "    N0 = 2000000\n",
    "    x,y,_ = generate_data(N=N0//2**l, D=3, T=2, beta0=beta0, beta=beta, alpha=alpha)\n",
    "    mu, sigma = laplace_approx(x, y, beta0, beta, alpha)\n",
    "    return conv_stats_dSUMO(x, y, beta0, beta, alpha, mu, sigma, level=l)\n",
    "L=13\n",
    "conv_stats = [tmp(l) for l in range(L)]\n",
    "conv_stats = pd.DataFrame(conv_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "plt.plot(conv_stats[['mean_abs_dSUMO', 'var_dSUMO', 'var_SUMO']])\n",
    "\n",
    "# plot O(2^{-l/2}), O(2^{-l}), O(2^{-2l})\n",
    "s,t = conv_stats[['mean_abs_dSUMO', 'var_dSUMO']].iloc[1]\n",
    "plt.plot(s*2.**(-np.arange(L)), c='grey')\n",
    "plt.plot(t*2.**(-np.arange(L)*2), c='grey')\n",
    "\n",
    "plt.legend([r'$\\mathrm{E}  |\\Delta^{\\mathrm{SUMO}}_{2^{\\ell}}|$', \n",
    "            r'$\\mathrm{Var}[\\Delta^{\\mathrm{SUMO}}_{2^{\\ell}}]$', \n",
    "            r'$\\mathrm{Var}[\\mathrm{SUMO}_{2^{\\ell}}]$', \n",
    "            r'$O(2^{-\\ell}), O(2^{-2\\ell})$'])\n",
    "plt.xlabel('Level')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "conv_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost comparison of objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "L = 13\n",
    "objectives = {\n",
    "    'NMC':      lambda x,y,mu,sigma,level: IWELBO(x, y, beta0, beta, alpha, mu, sigma, n_MC=2**level),\n",
    "    'MLMC':     lambda x,y,mu,sigma,level: IWELBO_MLMC(x, y, beta0, beta, alpha, mu, sigma, max_level=level, w0=0.95, b=1.8, randomize=False),\n",
    "    'RandMLMC': lambda x,y,mu,sigma,level: IWELBO_MLMC(x, y, beta0, beta, alpha, mu, sigma, max_level=level, w0=0.95, b=1.8, randomize=True),\n",
    "    'SUMO':     lambda x,y,mu,sigma,level: IWELBO_SUMO(x, y, beta0, beta, alpha, mu, sigma, K_max=2**level)\n",
    "}\n",
    "results = {'NMC':[], 'MLMC':[], 'RandMLMC':[], 'SUMO':[]}\n",
    "runtime = {'NMC':[], 'MLMC':[], 'RandMLMC':[], 'SUMO':[]}\n",
    "\n",
    "for name, obj in objectives.items():\n",
    "    \n",
    "    # evaluate variance\n",
    "    for i in range(100):\n",
    "        results[name].append([])\n",
    "        x,y,_ = generate_data(N=4000, D=3, T=2, beta0=beta0, beta=beta, alpha=alpha)\n",
    "        mu, sigma = laplace_approx(x, y, beta0, beta, alpha)\n",
    "        for level in range(L):\n",
    "            results[name][i].append( obj(x,y,mu,sigma,level).numpy() )\n",
    "    \n",
    "    # evaluate runtime\n",
    "    x,y,_ = generate_data(N=20000, D=3, T=2, beta0=beta0, beta=beta, alpha=alpha)\n",
    "    mu, sigma = laplace_approx(x, y, beta0, beta, alpha)    \n",
    "    for level in range(L):\n",
    "        if level>10 and name=='NMC':\n",
    "            start = time.time()\n",
    "            obj(*[vec[:200] for vec in [x,y,mu,sigma]], level)\n",
    "            end = time.time()\n",
    "            runtime[name].append((end - start)*100)\n",
    "        else:\n",
    "            start = time.time()\n",
    "            obj(x,y,mu,sigma,level)\n",
    "            end = time.time()\n",
    "            runtime[name].append(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for ests, rtime in zip(results.values(), runtime.values()):\n",
    "    var_per_recip_runtime = np.array(ests).var(axis=0) * np.array(rtime)\n",
    "    plt.plot(var_per_recip_runtime)\n",
    "plt.legend([name for name in results.keys()])\n",
    "plt.xlabel('Level')\n",
    "plt.ylabel('Variance per Reciprocal Runtime')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance per \"reciprocal of the runtime\" is considered because the varince is propotional to the  \"reciprocal of the runtime\". As we increase the computational complexity (runtime), the decrease in varince is inversely propotional to the complexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost comparison of gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "L = 13\n",
    "objectives = {\n",
    "    'NMC':      lambda x,y,beta0,beta,alpha,mu,sigma,level: IWELBO(x, y, beta0, beta, alpha, mu, sigma, n_MC=2**level),\n",
    "    'MLMC':     lambda x,y,beta0,beta,alpha,mu,sigma,level: IWELBO_MLMC(x, y, beta0, beta, alpha, mu, sigma, max_level=level, w0=0.95, b=1.8, randomize=False),\n",
    "    'RandMLMC': lambda x,y,beta0,beta,alpha,mu,sigma,level: IWELBO_MLMC(x, y, beta0, beta, alpha, mu, sigma, max_level=level, w0=0.95, b=1.8, randomize=True),\n",
    "    'SUMO':     lambda x,y,beta0,beta,alpha,mu,sigma,level: IWELBO_SUMO(x, y, beta0, beta, alpha, mu, sigma, K_max=2**level)\n",
    "}\n",
    "\n",
    "def d(f):\n",
    "    # return the derivative of f\n",
    "    # returned value is a function\n",
    "    def df(x,y,mu,sigma,level):\n",
    "        param = tf.concat([beta, [beta0], [alpha]], axis=0)\n",
    "        param = tf.Variable(param, dtype=tf.float64)\n",
    "        with tf.GradientTape(persistent=True) as g:\n",
    "            g.watch(param)\n",
    "            beta_ = param[:D]\n",
    "            beta0_ = param[D]\n",
    "            alpha_ = param[D+1]\n",
    "            target = f(x,y,beta0_,beta_,alpha_,mu,sigma,level)\n",
    "        est = g.gradient(target, param)\n",
    "        return est\n",
    "    return df\n",
    "\n",
    "results = {'NMC':[], 'MLMC':[], 'RandMLMC':[], 'SUMO':[]}\n",
    "runtime = {'NMC':[], 'MLMC':[], 'RandMLMC':[], 'SUMO':[]}\n",
    "\n",
    "for name, obj in objectives.items():\n",
    "    \n",
    "    # evaluate variance\n",
    "    for i in range(100):\n",
    "        results[name].append([])\n",
    "        x,y,_ = generate_data(N=4000, D=3, T=2, beta0=beta0, beta=beta, alpha=alpha)\n",
    "        mu, sigma = laplace_approx(x, y, beta0, beta, alpha)\n",
    "        for level in range(L):\n",
    "            results[name][i].append( d(obj)(x,y,mu,sigma,level).numpy() )\n",
    "    \n",
    "    # evaluate runtime\n",
    "    x,y,_ = generate_data(N=20000, D=3, T=2, beta0=beta0, beta=beta, alpha=alpha)\n",
    "    mu, sigma = laplace_approx(x, y, beta0, beta, alpha)    \n",
    "    for level in range(L):\n",
    "        # Avoid the memery runout by \n",
    "        # manipulating the case of NMC with large n_MC (large level)\n",
    "        if level>10 and name=='NMC':\n",
    "            start = time.time()\n",
    "            d(obj)(*[vec[:200] for vec in [x,y,mu,sigma]], level)\n",
    "            end = time.time()\n",
    "            runtime[name].append((end - start)*100)\n",
    "        else:\n",
    "            start = time.time()\n",
    "            d(obj)(x,y,mu,sigma,level)\n",
    "            end = time.time()\n",
    "            runtime[name].append(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for ests, rtime in zip(results.values(), runtime.values()):\n",
    "    var_per_recip_runtime = np.array(ests).var(axis=0).sum(axis=1) * np.array(rtime)\n",
    "    plt.plot(var_per_recip_runtime)\n",
    "plt.legend([name for name in results.keys()])\n",
    "plt.xlabel('Level')\n",
    "plt.ylabel(r'Variance ( tr(Cov) ) per Reciprocal Runtime')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood by Different Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_mlmc_cost(N, max_level, b, w0):\n",
    "    # compute the cost of MLMC estimation \n",
    "    # when the size of x (and that of y) is N\n",
    "    if max_level==0:\n",
    "        levels = np.array([0])\n",
    "        weights = np.array([1.])\n",
    "    else:\n",
    "        weights = 2.**(-(b+1)/2*np.arange(max_level))\n",
    "        weights /= sum(weights)\n",
    "        weights = np.concatenate([[w0], (1-w0)*weights])\n",
    "        levels = np.arange(max_level+1)\n",
    "    cost = np.ceil(N * weights[0])\\\n",
    "            + N * sum( np.ceil(weights[1:] * (2**levels[1:] + 2**(levels[1:]-1))) )\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "objectives = {\n",
    "    \"signorm\":   lambda x, y, beta0, beta, alpha, mu, sigma: sigmoid_normal_likelihood(x, y, beta0, beta, alpha),\n",
    "    \"elbo\":      lambda x, y, beta0, beta, alpha, mu, sigma: IWELBO(x, y, beta0, beta, alpha, mu, sigma, n_MC=1),\n",
    "    \"iwelbo8\":   lambda x, y, beta0, beta, alpha, mu, sigma: IWELBO(x, y, beta0, beta, alpha, mu, sigma, n_MC=8),\n",
    "    \"iwelbo64\":  lambda x, y, beta0, beta, alpha, mu, sigma: IWELBO(x, y, beta0, beta, alpha, mu, sigma, n_MC=64),\n",
    "    \"iwelbo512\": lambda x, y, beta0, beta, alpha, mu, sigma: IWELBO(x, y, beta0, beta, alpha, mu, sigma, n_MC=512),\n",
    "    \"iwelbo512_mlmc\": lambda x, y, beta0, beta, alpha, mu, sigma: IWELBO_MLMC(x, y, beta0, beta, alpha, mu, sigma, max_level=9, w0=0.90, b=1.8, randomize=False),\n",
    "    \"iwelbo512_randmlmc\": lambda x, y, beta0, beta, alpha, mu, sigma: IWELBO_MLMC(x, y, beta0, beta, alpha, mu, sigma, max_level=9, w0=0.90, b=1.8, randomize=True),\n",
    "    \"iwelbo512_sumo\": lambda x, y, beta0, beta, alpha, mu, sigma: IWELBO_SUMO(x, y, beta0, beta, alpha, mu, sigma, K_max=512),\n",
    "\n",
    "}\n",
    "N,T,D = (1000, 2, 3) if tf.test.is_gpu_available() else (200, 2, 3)\n",
    "\n",
    "n_repeat = 10\n",
    "params_repeated = {name:[] for name in objectives.keys()}\n",
    "\n",
    "for name, obj in objectives.items():\n",
    "    alpha_s = []\n",
    "    beta0_s = []\n",
    "    beta_s = []\n",
    "    for i in range(n_repeat):\n",
    "        print(\"training {}.... #iter:{} \".format(name,i))\n",
    "\n",
    "        beta0_ = tf.Variable(0., dtype=tf.float64)\n",
    "        beta_  = tf.Variable(np.zeros([D]), dtype=tf.float64)\n",
    "        alpha_   = tf.Variable(1., dtype=tf.float64)\n",
    "\n",
    "        # Gradient Descent\n",
    "        for t in range(2001):\n",
    "\n",
    "            rho_t = 0.5/(1+t)**0.7\n",
    "            x,y,_ = generate_data(N, D, T, beta0, beta, alpha)\n",
    "            # balance the cost of mlmc and nmc when level=9 (n_MC=512)\n",
    "            if 'mlmc' in name:\n",
    "                cost_nmc  = N * 2**9\n",
    "                cost_mlmc = get_mlmc_cost(N, max_level=9, b=1.8, w0=0.9)\n",
    "                N_mlmc = np.math.ceil(N * (cost_nmc / cost_mlmc))\n",
    "                x,y,_ = generate_data(N_mlmc, D, T, beta0, beta, alpha)    \n",
    "\n",
    "            with tf.GradientTape() as g:\n",
    "                g.watch([beta0_, beta_, alpha_])\n",
    "                mu, sigma = laplace_approx(x, y, beta0_.numpy(), beta_.numpy(), alpha_.numpy())\n",
    "                score = obj(x, y, beta0_, beta_, alpha_, mu, sigma)\n",
    "            dbeta0_, dbeta_, dalpha_ = g.gradient(score, [beta0_, beta_, alpha_])\n",
    "\n",
    "            beta0_ = beta0_ + rho_t*dbeta0_\n",
    "            beta_ = beta_ + rho_t*dbeta_\n",
    "            alpha_ = alpha_ + dalpha_\n",
    "            if t%200==0 and i==0:\n",
    "                print(\"#iter: {},\\tloss: {}\".format(t, -score.numpy()))\n",
    "        alpha_s.append(alpha_.numpy())\n",
    "        beta0_s.append(beta0_.numpy())\n",
    "        beta_s.append(beta_.numpy())\n",
    "    print()\n",
    "    params_repeated[name] = {\n",
    "            'alpha': np.array(alpha_s),\n",
    "            'beta0': np.array(beta0_s),\n",
    "            'beta': np.array(beta_s)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def expand(key, val):\n",
    "    # expand {\"name\":array([1,2,3,4,5])}\n",
    "    # into {\"name1\":1, \"name2\":2, ..., \"name5\":5}\n",
    "    if type(val)==np.ndarray:\n",
    "        return {key+str(i+1): x for i,x in enumerate(val)} \n",
    "    else:\n",
    "        return {key:val} \n",
    "\n",
    "def expand_param(param):\n",
    "    expanded_param = {}\n",
    "    for key, val in param.items():\n",
    "        expanded_param.update(expand(key,val))\n",
    "    return expanded_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "params = {'ground_truth': expand_param(param0)}\n",
    "params['ground_truth'].update({'MSE':0})\n",
    "for name in objectives.keys():\n",
    "    param_repeated = params_repeated[name]\n",
    "    param_mean   = expand_param({name: array.mean(axis=0) for name, array in  param_repeated.items()})\n",
    "    param_var = expand_param({name: array.std(axis=0) for name, array in  param_repeated.items()})\n",
    "    param = {name_:'{:.5f} ± {:.5f}'.format(mean,var**(1/2.)) \n",
    "             for name_, mean, var \n",
    "             in zip( param_mean.keys(), param_mean.values(), param_var.values() )}\n",
    "    error = [var+(mean-true_mean)**2 \n",
    "             for var, mean, true_mean \n",
    "             in zip( param_var.values(), param_mean.values(), params['ground_truth'].values() )]\n",
    "    MSE = sum(error)\n",
    "    param.update({'MSE':MSE})\n",
    "    params.update({name :param})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(params).T\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data.to_csv('../out/random_effect_logistic_regression/MLE_error_{}.csv'.format(timestamp()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bottom Line: \n",
    "- IWELBO with large number of inner MC samples gives better estiamte than ELBO or sigmoid normal integral approximation, even for this simple model. \n",
    "- MLMC is more effective than nested MC when used for maximum likelihood estimation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
